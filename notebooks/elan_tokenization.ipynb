{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELAN file tokenization with Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pympi can be installed with `pip install pympi-ling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pympi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELAN file is read into Pympi by giving the file path. Without that the result would be only an empty ELAN file, which can also be useful, for example, when adding content from some other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing unknown version of ELAN spec... This could result in errors...\n"
     ]
    }
   ],
   "source": [
    "elan_file = pympi.Eaf(file_path='../test.eaf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what kind of tiers there are for transcription type, or we could just get all the tiers, but usually we want to do something more focused:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orth@Niko', 'orth@Naknek']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiers = elan_file.get_tier_ids_for_linguistic_type('orthT')\n",
    "tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ref@Niko', 'orth@Niko', 'ref@Naknek', 'orth@Naknek'])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elan_file.get_tier_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['refT', 'orthT', 'wordT'])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elan_file.get_linguistic_type_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a linguistic type called `wordT`, but there is no tier for words. Let's add those! So for each tier which we picked up earlier we add a child tier. We pick from the parent tier the participant name and add that into the tier naming and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tier in tiers:\n",
    "    participant = elan_file.get_parameters_for_tier(tier)['PARTICIPANT']\n",
    "    elan_file.add_tier(tier_id = 'word@' + participant, parent = 'orth@' + participant, ling = 'wordT', part=participant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all tiers, which we can check by getting tier names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ref@Niko', 'orth@Niko', 'ref@Naknek', 'orth@Naknek', 'word@Niko', 'word@Naknek'])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elan_file.get_tier_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fine, but still bit boring! We can create new tiers, but it is more exciting to add there some content! So let's tokenize the transcription tier and add the content into word tier we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to tokenize different strings. It is worth noting that tokenization in itself is rather complex task, and I'm not going to the details here, but for ELAN corpora we may usually need to set up some more specific ways to handle it. Tokenization can be done both on sentence and word level, and there are lots of reasons why someone could it one way or another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sentence', '.', 'Тайӧ', 'мӧд', 'сёрникузя', '.']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize('This is a sentence. Тайӧ мӧд сёрникузя.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the next code block we have a more complicated for-loop, which takes for all transcription tier data, which includes start and end times, and then for each transcription content it tokenizes whatever there is and writes those pieces into the word level tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tier in tiers:\n",
    "    participant = elan_file.get_parameters_for_tier(tier)['PARTICIPANT']\n",
    "    content = elan_file.get_annotation_data_for_tier(tier)   \n",
    "    for start, end, content, reference in content:\n",
    "        for token_position, token in enumerate(wordpunct_tokenize(content)):\n",
    "            if token_position == 0:\n",
    "                elan_file.add_ref_annotation(value=token, id_tier='word@' + participant, tier2='ref@' + participant, time = start + 1)\n",
    "            else:\n",
    "                elan_file.add_ref_annotation(value=token, id_tier='word@' + participant, tier2='ref@' + participant, time = start + 1, prev='a' + str(elan_file.maxaid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result can now be saved into file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elan_file.to_file('tokenized_file.eaf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks like this:\n",
    "\n",
    "![Imgur](https://i.imgur.com/xtazRYn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks very nice, and the file works well. But if we examine it closed on XML level we find some conventions that deviate from the way ELAN would handle the file:\n",
    "\n",
    "![Imgur](https://i.imgur.com/DnZq2yh.png?2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is bit unclear to me whether this is a problem or not. It certainly is a problem if we want to read the file into R following the way annotation ID's are matching, as now they will have different pattern than before. And maybe there are some situations where ELAN uses the tier numbering for some of its own internal purposes, and this will lead into new problems? \n",
    "\n",
    "I just opened an [issue](https://github.com/dopefishh/pympi/issues/11) in Pympi GitHub repository, so maybe I'm not realizing something or this issue has another solution.\n",
    "\n",
    "Anyway, to wrap up this exercise, we can turn the code above into a new function that can be applied into other ELAN files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_eaf(filename):\n",
    "    elan_file = pympi.Eaf(file_path=filename)\n",
    "    tiers = elan_file.get_tier_ids_for_linguistic_type('orthT')\n",
    "\n",
    "    for tier in tiers:\n",
    "        participant = elan_file.get_parameters_for_tier(tier)['PARTICIPANT']\n",
    "        elan_file.add_tier(tier_id = 'word@' + participant, parent = 'orth@' + participant, ling = 'wordT', part=participant)\n",
    "    \n",
    "    for tier in tiers:\n",
    "        participant = elan_file.get_parameters_for_tier(tier)['PARTICIPANT']\n",
    "        content = elan_file.get_annotation_data_for_tier(tier)   \n",
    "        \n",
    "        for start, end, content, reference in content:\n",
    "            for token_position, token in enumerate(wordpunct_tokenize(content)):\n",
    "                if token_position == 0:\n",
    "                    elan_file.add_ref_annotation(value=token, id_tier='word@' + participant, tier2='ref@' + participant, time = start + 1)\n",
    "                else:\n",
    "                    elan_file.add_ref_annotation(value=token, id_tier='word@' + participant, tier2='ref@' + participant, time = start + 1, prev='a' + str(elan_file.maxaid))\n",
    "\n",
    "    elan_file.to_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with something like this we should be able to loop through a list of ELAN files and tokenize them. There are of course numerous questions and places for improvement:\n",
    "\n",
    "- Better tokenization method\n",
    "- Checking whether the tier exists\n",
    "- Checking whether the linguistic type exists\n",
    "- Should the tier be removed when it exist? Or renamed? Do we want a warning in this case, or what should happen?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
