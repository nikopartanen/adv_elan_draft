[
["index.html", "Advanced ELAN manipulation and analysis Section 1 Info 1.1 Course goals 1.2 Course structure 1.3 Resources 1.4 Practical info", " Advanced ELAN manipulation and analysis Niko Partanen 2017-09-05 Section 1 Info 1.1 Course goals I have been teaching on courses, informal workshops and meetings the basic use of ELAN and Praat. It seems to me that instructions of how these programs are used are included in many courses and summer schools, but it is not maybe so common to move beyond from that. In principle there is no need for this: once the researcher is familiar with the GUI, basic usage and search principles, there is not necessarily that much more to cover in the program itself. I strongly believe that a course of some week and intensive use of few more is enough to master ELAN. However, there are numerous ways to work programmatically with ELAN files, and this can be very useful both while producing the new data or analysing existing files. Although the focus is in ELAN, I will also start Praat from time to time. These two programs have somewhat different goals and niches, which are covered better in its own section. There is also a very different approach in these tools, since Praat can be very far manipulated through PraatScript, whereas with ELAN the means available are bit different. This course is not an introduction to ELAN or Praat, and it is neither an introduction to programming. Basic knowledge of R or Python will help a lot, but brave mind will probably be enough. I think we have to take as starting point that the majority of us are researchers first, and programming is neither our job or best skill. However, we can all learn to pay attention to some basic programming practices that make our work easier to adopt for others. These includes code comments and version control, among some other conventions. The main goal of this course is to help thinking about ways to automatize some parts of our workflows related to linguistic research. There are numerous tasks we do which demand hundreds and hundreds of clicks on mouse, and in all these situations we have to ask: are we spending time with something that can be automatized, or with something that demands our expert knowledge to be solved? We have to maximize the latter, also the time we spend when we try to get into that level of our work, instead of fighting against the cumbersome manual workflow where emphasis is easily in unnecessary parts of the task. Research necessarily is somewhat “boring”, it is inevitable that we do some thing thousands of times just to find out that we didn’t really learn that much. If we can find ways to speed up the manual parts of the process, we have the possibility to wander on even more unnecessary and poorly rewarding veins of though, which will eventually lead us to larger questions and their answers. 1.2 Course structure The course materials will be divided into several parts, and which all are used depends from the length of the course. Some parts can be skipped, and some can be used only as a reference. For example, the part about tools contains brief descriptions of the R and Python packages mainly discussed here with their most commonly used functions, so that can be a good place to look for help. I have also included there a list of useful references and links about basic usage of R and Python, just to get everyone onward. Introduction ELAN corpora Goals of this work Available tools Structure of the ELAN file XML structure Most needed XPath commands How ELAN interacts with its own XML? Python examples Creating new files with Pympi Manipulating ELAN file with Pympi R examples Interaction with emuR Interaction between Praat and R Creating new ELAN tiers in R Summary 1.3 Resources The whole course exists as an R package, which contains all functions discussed in the material. It can be installed with: # comment: fix this later library(devtools) install_github(&#39;langdoc/adv_elan&#39;) Besides the R package, the course repository also contains all Python code examples from the course. They are maybe also put into a module if there is time. The course materials also contain an example ELAN corpus with associated audio files. 1.4 Practical info Place: Freiburg, November 2017. More info will follow. "],
["intro.html", "Section 2 Introduction 2.1 Linguistic software 2.2 ELAN corpora", " Section 2 Introduction 2.1 Linguistic software It seems to me that in linguistic research there are roughly two different approaches to software. One is focused to programs with GUIs and interfaces, and assumes the user to do specific set of tasks with their data through this tool. There are two main problems with this approach: Actions done with mouse are impossible to record and repeat User is limited to actions (or their combinations) implemented in GUI In the worst cases the data is actually locked into the GUI so that the user can do nothing besides what is allowed there Another approach, arguably more common or vibrant at the moment, is to express the research procedures directly in programming languages, so that executing the code performs the wanted analysis. This approach also has its own issues, and it is never trivial to run old code on a new computer or after long time has passed, but there are many people working with these questions right now. One issue here is that there are many research tasks which need or are significantly simplified when there is a visual environment of some sort. Part of this is already solved by different methods of data visualization, but it is also possible to create more interactive environments. During the course we will go through several examples that are in different ways combining the programmatic automatized analysis into relatively shallow GUIs. It is rather easy nowadays to create a small interface in ad-hoc manner, as this doesn’t require very much time to set up. This differs radically from the traditional GUI perspective, since usually designing and building an user interface for anything has demanded well paid programmers to work for longer periods of time. 2.2 ELAN corpora There are some aspects of ELAN corpora that are very particular for this kind of data. Part of this comes from the fact that these corpora tend to be done with endangered minority languages. From this follows that there are very few Natural Language Processing tools that can be used readily out of the box with these languages. In this sense the NLP tools could often be called majority language processing tools, since even the most basic operations can get fiendishly complicated when we throw in very non-standard linguistic data. At least following traits seem to be common for ELAN corpora, although there are always exceptions too: Mainly spoken language data Audio and video media often available Relatively small size Integration into larger workflows with Toolbox or FLEx Part of the data may not be in ELAN files, or it may have done a round-trip somewhere else Lack of consistency ELAN has very few ways to monitor structural similarity between different ELAN files, and as they are often manually edited, the errors creep in Done with a small team over prolonged period of time Data is stored in ELAN XML, in the structure defined by researcher, not necessarily the person who tries to parse the file Machine readability may be an issue This makes use of ELAN corpora bit of a challenge when compared to large corpora on majority languages which may have been built with better resources. "],
["tools.html", "Section 3 Tools 3.1 R 3.2 Python 3.3 PraatScript 3.4 XPath", " Section 3 Tools This course has in its name both R and Python. I understand this can be criticized, as it is often mentioned that focusing into one or another would be “the best choice” in the long run. I think this thinking fails to understand the actual landscape these programming languages are located in, at least from the perspective of a linguist. I think many discussions about which programming language to focus into comes also from the perspective of professional programmers, and it sounds very plausible that longer and more concentrated work with one language eventually pays off in grandeous mastering of that one. However, for many of us the primary goal may be to get something to work. First thing to notice is that this ecosystem is on the move. The programming languages themselves are rather stable (well, R really is not), but there are continuously new packages and workflows that can be adapted into our uses. If something already exists in one language, but not in another, I think it is usually easiest to use the already finished and tested implementation. This is especially the case with more complex tasks which have a large amount of corner cases and questions that aren’t obvious in the beginning. In this vein, none of the exact methods in this course are meant as something that will be forever applicable as such, but especially in several years many things get outdated and there will be better and more elegant methods available. However, I think the basic ideas should be valid also in the longer run. There are also tools such as packrat for R and Anaconda or virtualenv for Python, which allow storing exact information about the environment where the code was run. Of course there are also available paths which are not at all touched here. For example, ELAN is written in Java, and the source code is available. It could be very useful to hack into that and extract some of the methods as their own more independent command line tools. ELAN code looks very nice and well written, so manipulating the tool directly should also be a real option and not very difficult for someone who knows Java well. Binding some of the native Java methods into R or Python functions could be a very safe way to manipulate ELAN files, as there would be no difference between this and GUI output. So my main idea here is that just use all tools that are available and which you bother to learn, and if there is something that gets too complicated, just hire someone who knows it to do it fast and effectively. But most importantly, if you do the latter, pay lots of attention to communication so that you all know what you want to be able to do. As far as I see, R and Python are both quite simple and easy to learn as interner is so full of resources, but of course both need more attention to be learned solidly. This, on the other hand, comes easiest after trying to build something you currently need. Maybe the most difficult part in programming nowadays is to figure out the ways how to call the things you want to do. Most of the problems we have are already solved, we just have to find the examples that can be adapted into our needs. In the beginning there are many difficulties, but this comes often from uncertainty of how to call what you want to do. 3.1 R The R NLP ecosystem is now changing very fast. One of the most interesting new developments is tidytext, which is an R package that allows working with text data in so called tidy R framework. This package contains very good function for tokenization, unnest_tokens(), and generally speaking it is worth looking into. The package authors have also written a book, Tidy text analysis with R. From ELAN perspective, the most useful package is certainly xml2. There is also an older package XML, but it is rather difficult to use in the end. 3.1.1 tidytext 3.1.2 xml2 3.2 Python Python package Pympi is certainly the most advanced tool currently to manipulate ELAN files programmatically. I think it touches well one of the most basic problems of interacting with the ELAN files: creating new tiers gets very difficult and dangerous. 3.2.1 pympi As a warning I must mention, that I think Pympi doesn’t in all cases follow the original ELAN specifications, and the files created with it differ slightly from the ones created when same is done through ELAN GUI. This can be fine, and I don’t think there are parts that do not work, but in some sense it is always good to keep this in mind. We do not know if non-standard structures in some places are always accepted by all ELAN versions. 3.3 PraatScript When we work with extracting information from the audio files, the situation is commonly that Praat can do big part of the analysis already, and if it can be done in Praat, it can be automatized with PraatScript. This can be executed from R or Python as well. There are also R and Python packages for interacting with Praat, but as far as I see, these are also usually bound to PraatScript in the end, and using them tends to result in complex mixture of the programming language and PraatScript. This surely works when you know both of those well, and I will also study these packages further, but for now I have found it cleaner to keep these two separated. There are few reasons: It is easier to find help for PraatScript or the programming language alone This way PraatScript is easier to reuse for people who just want to deal with PraatScript This said, I don’t really know very much about these packages, so if someone has good experiences, please let me know! I guess my main point is that PraatScript is really useful, and if you do something repeatedly in Praat, please check it and see if it can be adapted into your use. One of the best introductions to the topic is here. 3.4 XPath Although XML is usually considered a human readable file format, I personally would advice against modifying XML directly in the text editor unless there is a specific case what you know you want to do there. The most advanced tool to work with XML files is XSLT. Unfortunately XSLT is very difficult to use and learn. However, there is one part of these core XML technologies which we inevitably need: XPath. It is a small language of its own that can be used to select parts of XML file. It also has a number of functions of its own. 3.4.1 Examples //node = select any node with this name //node/@attribute = select an attribute //node[@attribute='something'] = select node which has an attribute with value //node[@attribute='starts-with(someth)'] = select node which has an attribute which starts with ./child/child = move down to child ./../.. = move up to parent In xml2 R package the XPath expression goes into R function xml_find_all(). With function xml_text() we can retrieve the text of currently selected nodes. suppressPackageStartupMessages(library(tidyverse)) library(xml2) read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;wordT&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;) ## {xml_nodeset (2)} ## [1] &lt;ANNOTATION_VALUE&gt;words&lt;/ANNOTATION_VALUE&gt; ## [2] &lt;ANNOTATION_VALUE&gt;here&lt;/ANNOTATION_VALUE&gt; read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;wordT&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;) %&gt;% xml_text() ## [1] &quot;words&quot; &quot;here&quot; It is important to understand that xml_find_all() function selects the nodes, but all adjacent nodes are still present in the tree. This is demonstrated below: read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;wordT&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;) %&gt;% xml_find_all(&quot;../../..&quot;) %&gt;% xml_attr(&#39;PARTICIPANT&#39;) ## [1] &quot;Niko&quot; So although we have selected something, we can still access all other content in the tree. "],
["elan-file-structure.html", "Section 4 ELAN file structure 4.1 Minimal file 4.2 Tier type naming convention 4.3 Hierarchies 4.4 Discussion", " Section 4 ELAN file structure In this part we go into deeper details of ELAN XML structure. It is important to understand how ELAN stores information into different tiers, so that we can easily extract the parts we need. It is also very good test for data integrity to parse all the data from ELAN file, since this verifies that all data is stored in retrievable manner. 4.1 Minimal file &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;ANNOTATION_DOCUMENT AUTHOR=&quot;&quot; DATE=&quot;2017-09-05T14:53:24+06:00&quot; FORMAT=&quot;3.0&quot; VERSION=&quot;3.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:noNamespaceSchemaLocation=&quot;http://www.mpi.nl/tools/elan/EAFv3.0.xsd&quot;&gt; &lt;HEADER MEDIA_FILE=&quot;&quot; TIME_UNITS=&quot;milliseconds&quot;&gt; &lt;MEDIA_DESCRIPTOR MEDIA_URL=&quot;file:///Users/niko/audio.wav&quot; MIME_TYPE=&quot;audio/x-wav&quot; RELATIVE_MEDIA_URL=&quot;../../audio.wav&quot;/&gt; &lt;PROPERTY NAME=&quot;URN&quot;&gt;urn:nl-mpi-tools-elan-eaf:7e5bf3cc-a72f-4868-810b-2ca34876e64c&lt;/PROPERTY&gt; &lt;PROPERTY NAME=&quot;lastUsedAnnotationId&quot;&gt;5&lt;/PROPERTY&gt; &lt;/HEADER&gt; &lt;TIME_ORDER&gt; &lt;TIME_SLOT TIME_SLOT_ID=&quot;ts1&quot; TIME_VALUE=&quot;0&quot;/&gt; &lt;TIME_SLOT TIME_SLOT_ID=&quot;ts2&quot; TIME_VALUE=&quot;1000&quot;/&gt; &lt;/TIME_ORDER&gt; &lt;TIER LINGUISTIC_TYPE_REF=&quot;refT&quot; PARTICIPANT=&quot;Niko&quot; TIER_ID=&quot;ref@Niko&quot;&gt; &lt;ANNOTATION&gt; &lt;ALIGNABLE_ANNOTATION ANNOTATION_ID=&quot;a1&quot; TIME_SLOT_REF1=&quot;ts1&quot; TIME_SLOT_REF2=&quot;ts2&quot;&gt; &lt;ANNOTATION_VALUE&gt;.001&lt;/ANNOTATION_VALUE&gt; &lt;/ALIGNABLE_ANNOTATION&gt; &lt;/ANNOTATION&gt; &lt;/TIER&gt; &lt;TIER LINGUISTIC_TYPE_REF=&quot;orthT&quot; PARENT_REF=&quot;ref@Niko&quot; PARTICIPANT=&quot;Niko&quot; TIER_ID=&quot;orth@Niko&quot;&gt; &lt;ANNOTATION&gt; &lt;REF_ANNOTATION ANNOTATION_ID=&quot;a2&quot; ANNOTATION_REF=&quot;a1&quot;&gt; &lt;ANNOTATION_VALUE&gt;words here&lt;/ANNOTATION_VALUE&gt; &lt;/REF_ANNOTATION&gt; &lt;/ANNOTATION&gt; &lt;/TIER&gt; &lt;TIER LINGUISTIC_TYPE_REF=&quot;wordT&quot; PARENT_REF=&quot;orth@Niko&quot; PARTICIPANT=&quot;Niko&quot; TIER_ID=&quot;word@Niko&quot;&gt; &lt;ANNOTATION&gt; &lt;REF_ANNOTATION ANNOTATION_ID=&quot;a4&quot; ANNOTATION_REF=&quot;a2&quot;&gt; &lt;ANNOTATION_VALUE&gt;words&lt;/ANNOTATION_VALUE&gt; &lt;/REF_ANNOTATION&gt; &lt;/ANNOTATION&gt; &lt;ANNOTATION&gt; &lt;REF_ANNOTATION ANNOTATION_ID=&quot;a5&quot; ANNOTATION_REF=&quot;a2&quot; PREVIOUS_ANNOTATION=&quot;a4&quot;&gt; &lt;ANNOTATION_VALUE&gt;here&lt;/ANNOTATION_VALUE&gt; &lt;/REF_ANNOTATION&gt; &lt;/ANNOTATION&gt; &lt;/TIER&gt; &lt;LINGUISTIC_TYPE GRAPHIC_REFERENCES=&quot;false&quot; LINGUISTIC_TYPE_ID=&quot;refT&quot; TIME_ALIGNABLE=&quot;true&quot;/&gt; &lt;LINGUISTIC_TYPE CONSTRAINTS=&quot;Symbolic_Association&quot; GRAPHIC_REFERENCES=&quot;false&quot; LINGUISTIC_TYPE_ID=&quot;orthT&quot; TIME_ALIGNABLE=&quot;false&quot;/&gt; &lt;LINGUISTIC_TYPE CONSTRAINTS=&quot;Symbolic_Subdivision&quot; GRAPHIC_REFERENCES=&quot;false&quot; LINGUISTIC_TYPE_ID=&quot;wordT&quot; TIME_ALIGNABLE=&quot;false&quot;/&gt; &lt;CONSTRAINT DESCRIPTION=&quot;Time subdivision of parent annotation&#39;s time interval, no time gaps allowed within this interval&quot; STEREOTYPE=&quot;Time_Subdivision&quot;/&gt; &lt;CONSTRAINT DESCRIPTION=&quot;Symbolic subdivision of a parent annotation. Annotations refering to the same parent are ordered&quot; STEREOTYPE=&quot;Symbolic_Subdivision&quot;/&gt; &lt;CONSTRAINT DESCRIPTION=&quot;1-1 association with a parent annotation&quot; STEREOTYPE=&quot;Symbolic_Association&quot;/&gt; &lt;CONSTRAINT DESCRIPTION=&quot;Time alignable annotations within the parent annotation&#39;s time interval, gaps are allowed&quot; STEREOTYPE=&quot;Included_In&quot;/&gt; &lt;/ANNOTATION_DOCUMENT&gt; This is one file with three tiers. The first is reference tier, containing the ID for the utterance, the second is the transcription, and the last one contains the tokenized words. It is the kind of structure used in language documentation projects connected to Freiburg. Other structures are certainly possible, although some kind of hierarchy between tiers is generally encouraged. At least following guidelines can be distinguished: Each content type should have its own tier type If there is a hierarchical relation, it should be used instead of independent tiers 4.1.1 Participant name convention Some parts are more a matter of preferred convention. Each tier contains information about participants in two possible places: PARTICIPANT=&quot;Niko&quot; TIER_ID=&quot;ref@Niko This makes a difference in which convention has to be used in order to select the nodes of this speaker. 4.2 Tier type naming convention The situation is similar with selecting all tiers of specific type: LINGUISTIC_TYPE_REF=&quot;refT&quot; TIER_ID=&quot;ref@Niko&quot; Also here the tier type is specified in both the type name and as part of the tier ID. So the following commands are in this case identical: suppressPackageStartupMessages(library(tidyverse)) library(xml2) read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;wordT&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;) ## {xml_nodeset (2)} ## [1] &lt;ANNOTATION_VALUE&gt;words&lt;/ANNOTATION_VALUE&gt; ## [2] &lt;ANNOTATION_VALUE&gt;here&lt;/ANNOTATION_VALUE&gt; read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[starts-with(@TIER_ID, &#39;word&#39;)]/ANNOTATION/*/ANNOTATION_VALUE&quot;) ## {xml_nodeset (2)} ## [1] &lt;ANNOTATION_VALUE&gt;words&lt;/ANNOTATION_VALUE&gt; ## [2] &lt;ANNOTATION_VALUE&gt;here&lt;/ANNOTATION_VALUE&gt; In the situations where every content type doesn’t have its own type, it may be more useful to select them by part of the tier ID. 4.3 Hierarchies Very often we want to have the time spans of different units, even when we are dealing with symbolically subdivided words that don’t have this information for the units themselves. Only the highermost tier in ELAN has the information about the time codes, so associating lower level tiers with their times usually demands reconstructing the hierarchies in the file. 4.4 Discussion The conventions specified above define how we are supposed to extract the wanted information. Usually this is done with the goal to get all content of all speakers from the tier X. We want all utterances, or all words. However, often we would like to have a file that contains larger amount of data in one structure. "],
["parsing-elan-files-to-r.html", "Section 5 Parsing ELAN files to R 5.1 Parsing with FRelan package", " Section 5 Parsing ELAN files to R The kind of structures I have usually been working with in R are like this: Note: Update the example to have better column names This is one place where standardized and cross-comparable metadata would be useful. The idea behind this structure is that we can consider each token as one observation, and this format with one token per row allows easy plotting and statistical testing without issues. I have written in this blog about more exact conventions in parsing ELAN files, especially within FRelan R package that I have built to work with Freiburg project ELAN files. However, I have not been very succesful in getting some system to work so that it would be easily and intuitively adapted into project with an entirely different tier structure. Probably the easiest way is to modify the code every time there is a new tier structure. 5.1 Parsing with FRelan package The function I have in the FRelan package for parsing one tier looks like this: FRelan::read_tier ## function (eaf_file = &quot;/Volumes/langdoc/langs/kpv/kpv_izva20140404IgusevJA/kpv_izva20140404IgusevJA.eaf&quot;, ## linguistic_type = &quot;wordT&quot;, read_file = T, xml_object = F) ## { ## `%&gt;%` &lt;- dplyr::`%&gt;%` ## if (read_file == F) { ## file = xml_object ## } ## else { ## file &lt;- xml2::read_xml(eaf_file) ## } ## participants_in_file &lt;- file %&gt;% xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, ## linguistic_type, &quot;&#39;]&quot;)) %&gt;% xml2::xml_attr(&quot;PARTICIPANT&quot;) ## coerce_data_frame &lt;- function(participant) { ## dplyr::data_frame(content = file %&gt;% xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, ## linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, participant, ## &quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;)) %&gt;% xml2::xml_text(), ## annot_id = file %&gt;% xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, ## linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, participant, ## &quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/..&quot;)) %&gt;% xml2::xml_attr(&quot;ANNOTATION_ID&quot;), ## ref_id = file %&gt;% xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, ## linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, participant, ## &quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/..&quot;)) %&gt;% xml2::xml_attr(&quot;ANNOTATION_REF&quot;), ## participant = file %&gt;% xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, ## linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, participant, ## &quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/../../..&quot;)) %&gt;% ## xml2::xml_attr(&quot;PARTICIPANT&quot;), tier_id = file %&gt;% ## xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, ## linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, participant, ## &quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/../../..&quot;)) %&gt;% ## xml2::xml_attr(&quot;TIER_ID&quot;), type = file %&gt;% xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, ## linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, participant, ## &quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/../../..&quot;)) %&gt;% ## xml2::xml_attr(&quot;LINGUISTIC_TYPE_REF&quot;), time_slot_1 = file %&gt;% ## xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, ## linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, participant, ## &quot;&#39;]/ANNOTATION/*&quot;)) %&gt;% xml2::xml_attr(&quot;TIME_SLOT_REF1&quot;), ## time_slot_2 = file %&gt;% xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, ## linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, participant, ## &quot;&#39;]/ANNOTATION/*&quot;)) %&gt;% xml2::xml_attr(&quot;TIME_SLOT_REF2&quot;)) ## } ## plyr::ldply(participants_in_file, coerce_data_frame) %&gt;% ## dplyr::tbl_df() ## } ## &lt;environment: namespace:FRelan&gt; It works fine, but I have not yet updated it to use map() function from purrr package. It also has some extra parts in it to parse either an XML file or an XML file already read into memory. We usually want to read several tiers and merge them together, and in this context it is not good to open the XML file again every time we read it, as this would slow the function down very much. "],
["manipulating-elan-files-with-pympi.html", "Section 6 Manipulating ELAN files with Pympi", " Section 6 Manipulating ELAN files with Pympi Pympi is a very mature Python library and using it is enjoyable. However, I myself usually feel the most comfortable when I have the data in an R data frame, so I often fall back into that no matter what. I strongly encourage to test further data manipulation and analysis in Python, I assume that especially pandas should be very useful. "],
["pympi-examples.html", "Section 7 Pympi examples 7.1 Creating a new ELAN file 7.2 Populating the ELAN file with content 7.3 Merging ELAN files", " Section 7 Pympi examples 7.1 Creating a new ELAN file 7.2 Populating the ELAN file with content 7.3 Merging ELAN files "],
["shiny-components.html", "Section 8 Shiny components 8.1 DT 8.2 Leaflet 8.3 ggplot2 8.4 Advantages and disadvantages of Shiny", " Section 8 Shiny components The easiest way to build a new Shiny application is to find the pieces you need from Shiny Gallery and start adjusting from this. Shiny applications and their functions differ from normal R functions in what are called reactive functions. These functions are constructed so that they can get their arguments interactively from the UI part of Shiny code. Grasping how the reactive functions work is little bit challenging, but once you get touch to it the basic concept will not change. The Shiny application has two parts. They are called ui and server. The code is executed on the server side, and the variables that the user manipulates are on UI side. The results computed on the server code will again be placed as output functions to the UI, which creates clear interaction between these parts. It is possible to have the Shiny app in two different files called ui.R and server.R, but they can also be in one function. With more complicated applications it is often easier to put them into different files, but there are also situations where compactness of one file can make it easier to follow. I often do the first prototype into one function and split it later when I start adding bells and whistles. In this point it is good to mention, that there is a specific set of HTML Widgets that interact very well with Shiny. They are different from Shiny applications as they are just plain HTML and JavaScript and can be opened in any browser, and they also can be interactive, but the interactivity is usually somewhat limited. Shiny can be used to tie easily two distinct processes into one another. There are new HTML Widgets coming all the time, and apparently creating new ones is not too difficult. The idea in an HTML Widget is that an R object is transformed into HTML which contains JavaScript. Thereby it is not necessary to write JavaScript ourselves, but we get to benefit from large variety of already well working and pretty visualizations. These are called JavaScript library bindings. The concept exists also in Python. This comes with a caveout that if something in the JavaScript library is not yet implemented in the binding R function, it can be quite tricky to get it work the intended way without touching more into JavaScript. JavaScript is nice language, and there is a good introduction JavaScript for Cats, but this can still be something that takes unexpectedly much time. So if your supervisor or colleague sees your HTML Widget or Shiny application, they may suggest different kinds of additions. Do not say yes before googling a bit. The situation is paradoxical in the sense that very impressive presentations can be created with few lines of code, but a small adjustment can be very difficult to set up. Next I will present the HTML Widgets which in my opinion have been the most useful while working with linguistic data. They are also used in later examples. 8.1 DT datatables is a JavaScript library that can be used to make very nice looking tables. In principle a KWIC style concordance is also just a table with specific formatting, as seen in Example… 8.2 Leaflet This is the best package for creating interactive maps at the moment. This is demonstrated in Example… 8.3 ggplot2 This is kind of an old work horse, it is probably the “basic” plotting library in R nowadays, and having familiarity with this will come handy later. It is also easy to make it interact with Shiny, as seen in Example… 8.4 Advantages and disadvantages of Shiny If the concept you have in mind is simple enough to be realized as an HTML Widget, then that is a better way to go. Those can be hosted online anywhere, for example in GitHub, and they work very reliably on any browser. The trick here is that HTML Widget executes only JavaScript in the end, so using it doesn’t differ from any other website which has some JavaScript based visualizations. With Shiny applications the situation is more complicated. They can be ran locally, and then the application can also easily access all local files. They can also be ran on server, but this needs bit more work as it doesn’t seem to be so trivial to set up one, although there are good instructions online. The most common way to host a Shiny application online is probably through RStudio owned website shinyapps.io, and this is usually a very sufficient alternative. On the free account there are limits into how much the application can be used in a month, but for small number of users this shouldn’t be any problem. Of course this all leaves a nagging feeling, as it is always possible to come up with something that has more vivid number of users, and I’m rather sure in this case the free account would run out of allocated minutes very fast. One good midway is to host the application in GitHub, and instruct the user to run it directly from there on their own computer with a function shiny::runGitHub. This is not the same as having it available online for everybody, but it is one way which I have preferred. To be clear, the reason why the Shiny application has to be hosted on the server is that there R code being executed while you use it, and this is something web browser doesn’t handle. I assume the functionality of any Shiny application could be replicated in JavaScript, and then it would work in browser as such, but I can’t even imagine how many lines of code that would eventually demand. This leads to the one of the key aspects of Shiny: rapid prototyping time. Once you know the basic logic of the app, it is possible to implement entirely new kind of interface in a matter of hours. If we spend some time in standardizing our data, so that we all use same concepts when we speak refer to metadata and different parts of our annotations, I could imagine many parts of code can be freely reused as such. "],
["example-interaction-with-emur.html", "Section 9 Example: Interaction with emuR 9.1 Procedure", " Section 9 Example: Interaction with emuR This is a bit specific example and basically just copies the example code from emuR package documentation. This R package connects into tools set up in the University of Munchen. What we are doing here is so called forced alignation. This means automatic matching of text and speech signal. The problem here is that I don’t consider forced alignation to be satisfactorily solved for our use at the moment, not in the way the method is normally used. The tools normally assume that there is 100% correspondence between the transcribed text and audio, and try to align that as exactly as possible. This works well, but this kind of correspondence is much rarer than we would hope for. It is very common that transcriptions have been edited and the original audio contains only some parts of the text, or have segments which are missing from the transcriptions. I haven’t found yet a tool that could do forced alignation well enough with the transcriptions and recordings I have at hand from archives and old publications. The situation is different when we adjust the scale, on small segments the performance is better. And the way forced alignation is done is often such that we get lots of extra information: word and phoneme boundaries. This way forced alignation can well be used with the file typically contained in ELAN files. 9.1 Procedure In this example we use a script that cuts each utterance in the ELAN file into a new WAV-file, and saves the content of the transcription as a new text file. These two files will be matched by their names. This gives us small snippets of WAV files, which still correspond exactly with what we have in the ELAN file. There are inherent problems in taking data out from ELAN this way. What if we decide to change the transcription in original ELAN file? We are going to derive a bunch of new files, and do we also have to keep those synchronized somehow? The main question is how we make sure that we know into what data we are referring to later on. I leave this question unanswered now, but I have to emphasize that this is something we can’t avoid thinking. So the steps we need to do are: Find all utterances Split file by those boundaries and save the results Save the utterance text files Send those to Munchen with emuR Manipulate the emuR result until you have nice Praat files After this it is necessary to fix the segmentation, as it is certainly not perfect. It can be pretty good with a language officially supported by MAUS tools, but with languages like Komi we have to do all kinds of ugly hacks to get it work. Anyway the amount of work is reduced significantly, and we can focus into the most important part of it: deciding with our expert knowledge where the correct phoneme boundaries are. "],
["example-interaction-with-praat.html", "Section 10 Example: Interaction with Praat 10.1 Research questions 10.2 Implementation 10.3 Shiny application 10.4 Observations 10.5 Exercise", " Section 10 Example: Interaction with Praat This example needs data in Praat files with annotation on phoneme level. This is very time consuming to create, but as we saw in Example 1, we can get almost readily usable data through already existing tools using the forced alignment methods offered by Munchen University. So the data we have in hand looks like this: 10.1 Research questions Out of the annotated vowels, which are correctly analysed and which are somehow wrong? How do the vowel formants plot Can we very easily identify the wrongly analysed ones and correct those? 10.2 Implementation We are using several smaller tools here: sendpraat to communicate with Praat PraatScript to extract the formants and write .formant files R to read those files and reshape those into a nice dataframe R to plot the output Shiny R package to wrap this into a shallow interface 10.3 Shiny application The application looks like below. It has only a few functionalities: Select the vowel by clicking it See what is selected under the plot Open the current selection in Praat Play the word where the currently selected vowel is Filter by length (not really useful in this case, but worth checking) Some of these are compromises in a way. It is easy to set up the file to open in Praat, but this of course works only locally. It is not possible to deploy an app like this online. This leads into new questions. How much effort should we pay to make the tool maximally reusable? The question is not trivial, as setting up it more complexly would easily lead into tens of hours spent with adjusting things. In some cases it is useful just to say that this works for me, this is how I use it, this is enough for now. Put the code into GitHub and hope someone else fixes the harder problems. 10.4 Observations There are vowel reductions in faster speech which are not very well described in Komi It seems that the Russian and Komi vowels plot differently, do they? 10.5 Exercise How would you test if the Russian and Komi vowels plot differently? Hints: Skip for now the linguistic part of the question, just think how you get this information into the program. Also skip the statistical part of how you actually verify what you have, this is very simple exercise which is supposed to illustrate moving data in and out. "],
["example-concordances-and-map.html", "Section 11 Example: Concordances and map 11.1 Use", " Section 11 Example: Concordances and map This example is very simple and involves some manipulation of data table object created with DT package. Idea is to create a simple search interface, which can be used conveniently in the end of longer command with the pipe operator. Since DT package outputs normal JavaScript object, there are numerous ways to manipulate it with JavaScript besides R. I’m not going into this now. This application also illustrates some ways to add new information into different tabs. Because the object displayed in the table is just a data frame that has gone through the filtering specified in the boxes above, it is also possible to visualize data that is part of the same data frame, even when it is not present in that table. So in this case all the data comes also with coordinates, which are mapped with the package leaflet. 11.1 Use corpus %&gt;% filter(gender == &#39;M&#39;) %&gt;% FRorpus() "],
["final-words.html", "Section 12 Final Words", " Section 12 Final Words We are living in very interesting times where many technologies are just making their appearance. It is certain that our work practices will also shift and change. If we are working with smaller or less standardized languages, the things will certainly take their time before everything is available for us as well. It is thereby hard to draw a line between the problems which we can label solved and unsolved, because even if something is considered solved, it may not work at all with our languages. However, I think following parts we commonly work with will in some point be automatically done: Segmentation Speaker recognition Speech recognition POS tagging Syntactic annotation Named Entity Recognition … What is important to see here, is that this course did not touch any of these topics as such, but we have been only discussing the possible ways to analyse and manipulate the data which we have got one way or another, usually through longer manual work. Having more data automatically will not change the need to be able to manipulate and analyse the data we have, on contrary, it certainly will require even more knowledge about these issues than now, as the datasets become too large to be manually adjusted or examined. "],
["references.html", "References", " References "]
]
