[
["index.html", "Advanced ELAN manipulation and analysis Section 1 Info 1.1 Course goals 1.2 Course structure 1.3 Schedule 1.4 Resources 1.5 Practical info", " Advanced ELAN manipulation and analysis Niko Partanen 2017-11-03 Section 1 Info 1.1 Course goals I have been teaching on courses, informal workshops and meetings the basic use of ELAN and Praat. It seems to me that instructions of how these programs are used are included in many courses and summer schools, but it is not maybe so common to move beyond from that. In principle there is no need for this: once the researcher is familiar with the GUI, basic usage and search principles, there is not necessarily that much more to cover in the program itself. I strongly believe that a course of some week and intensive use of few more is enough to master ELAN. On the other hand, many courses and handbooks focus into statistical analysis of linguistic data. I think there is a clear need for more discussion about the ways how do we get our linguistic data most effectively into statistical or analytical software we intend to use. This may sound like a topic that is not worth that much thought, but as is often said that 80% of data analysis is data manipulation, the topic is eventually more central for our daily work than we may think. There are numerous ways to work programmatically with ELAN files, and this can be very useful both while producing the new data or analysing existing files. Although the focus is in ELAN, I will also mention Praat from time to time. These two programs have somewhat different goals and niches, which are covered better in its own section. There is also a very different approach in these tools, since Praat can be very far manipulated through PraatScript, whereas with ELAN the means available are bit different. This course is not an introduction to ELAN or Praat, and it is neither an introduction to programming. Basic knowledge of R or Python will help a lot, but brave mind will probably be enough. I think we have to take as starting point that the majority of us are researchers first, and programming is neither our job or best skill. However, we can all learn to pay attention to some basic programming practices that make our work easier to adopt for others. These includes code comments and version control, among some other conventions. The main goal of this course is to help thinking about ways to automatize some parts of our workflows related to linguistic research. There are numerous tasks we do which demand hundreds and hundreds of clicks on mouse, and in all these situations we have to ask: are we spending time with something that can be automatized, or with something that demands our expert knowledge to be solved? We have to maximize the latter, also the time we spend when we try to get into that level of our work, instead of fighting against the cumbersome manual workflow where emphasis is easily in unnecessary parts of the task. Research necessarily is somewhat “boring”, it is inevitable that we do some thing thousands of times just to find out that we didn’t really learn that much. If we can find ways to speed up the manual parts of the process, we have the possibility to wander on even more unnecessary and poorly rewarding veins of though, which will eventually lead us to larger questions and their answers. 1.2 Course structure The course materials will be divided into several parts, and which all are used depends from the length of the course. Some parts can be skipped, and some can be used only as a reference. For example, the part about tools contains brief descriptions of the R and Python packages mainly discussed here with their most commonly used functions, so that can be a good place to look for help. I have also included there a list of useful references and links about basic usage of R and Python, just to get everyone onward. Introduction ELAN corpora Goals of this work Available tools Structure of the ELAN file XML structure How ELAN interacts with its own XML? Python examples Creating new files with Pympi Manipulating ELAN file with Pympi R examples Interaction with emuR Interaction between Praat and R Creating new ELAN tiers in R Summary 1.3 Schedule Thu 16.11. FRIAS 09:30-11:00 Course Topic: ELAN corpora &amp; Goals of programmatic workflows &amp; ELAN file 11:00-11:30 Coffee 11:30-13:00 Course Topic: Available ecosystem (R, Python, ELAN, Praat) 13:00-14:00 Lunch (on own expenses) 14:00-15:30 Course Topic: Parsing ELAN file + metadata 15:30-16:00 Coffee 16:00-17:30 Course Topic: Further interaction with ELAN corpus in R Thu 16.11. University 18:00–20:00 Guest lecture by Mark Davies “Examining variation and change in language and culture with large online corpora” Fr 17.11. FRIAS 09:30-11:00 Course Topic: ELAN tier manipulation with Python 11:00-11:30 Coffee 11:30-13:00 Course Topic: Examples of tools in interaction 13:00-14:00 Lunch (on own expenses) 14:00-15:30 Course Topic: Summary 1.4 Resources The whole course exists as an R package, which contains all functions discussed in the material. It can be installed with: # comment: fix this later library(devtools) install_github(&#39;langdoc/adv_elan&#39;) Besides the R package, the course repository also contains all Python code examples from the course. They are maybe also put into a module if there is time. The course materials also contain an example ELAN corpus with associated audio files. 1.5 Practical info I will teach with these materials, or their subset, in a workshop in Freiburg, around 16.-17. November 2017. "],
["intro.html", "Section 2 Introduction 2.1 Elan and Praat – strengths and weaknesses 2.2 Linguistic software 2.3 ELAN corpora 2.4 Preprocessing 2.5 Analysis workflows 2.6 When to write back to ELAN file 2.7 Annotations as an independent dataset", " Section 2 Introduction ## Loading tidyverse: ggplot2 ## Loading tidyverse: tibble ## Loading tidyverse: tidyr ## Loading tidyverse: readr ## Loading tidyverse: purrr ## Loading tidyverse: dplyr ## Conflicts with tidy packages ---------------------------------------------- ## filter(): dplyr, stats ## lag(): dplyr, stats ## Linking to GEOS 3.6.1, GDAL 2.1.3, proj.4 4.9.3 2.1 Elan and Praat – strengths and weaknesses 2.2 Linguistic software It seems to me that in linguistic research there are roughly two different approaches to software. One is focused to programs with GUIs and interfaces, and assumes the user to do specific set of tasks with their data through this tool. There are two main problems with this approach: Actions done with mouse are impossible to record and repeat User is limited to actions (or their combinations) implemented in GUI In the worst cases the data is actually locked into the GUI so that the user can do nothing besides what is allowed there Another approach, arguably more common or vibrant at the moment, is to express the research procedures directly in programming languages, so that executing the code performs the wanted analysis. This approach also has its own issues, and it is never trivial to run old code on a new computer or after long time has passed, but there are many people working with these questions right now. One issue here is that there are many research tasks which need or are significantly simplified when there is a visual environment of some sort. Part of this is already solved by different methods of data visualization, but it is also possible to create more interactive environments. During the course we will go through several examples that are in different ways combining the programmatic automatized analysis into relatively shallow GUIs. It is rather easy nowadays to create a small interface in ad-hoc manner, as this doesn’t require very much time to set up. This differs radically from the traditional GUI perspective, since usually designing and building an user interface for anything has demanded well paid programmers to work for longer periods of time. 2.3 ELAN corpora There are some aspects of ELAN corpora that are very particular for this kind of data. Part of this comes from the fact that these corpora tend to be done with endangered minority languages. From this follows that there are very few Natural Language Processing tools that can be used readily out of the box with these languages. In this sense the NLP tools could often be called majority language processing tools, since even the most basic operations can get fiendishly complicated when we throw in very non-standard linguistic data. At least following traits seem to be common for ELAN corpora, although there are always exceptions too: Mainly spoken language data Audio and video media often available Relatively small size Integration into larger workflows with Toolbox or FLEx Part of the data may not be in ELAN files, or it may have done a round-trip somewhere else Lack of consistency ELAN has very few ways to monitor structural similarity between different ELAN files, and as they are often manually edited, the errors creep in Done with a small team over prolonged period of time Data is stored in ELAN XML, in the structure defined by researcher, not necessarily the person who tries to parse the file Machine readability may be an issue This makes use of ELAN corpora bit of a challenge when compared to large corpora on majority languages which may have been built with better resources. 2.4 Preprocessing I often write about automatic preprocessing of data. By this I refer to practices we can do on basis of already existing annotations, usually with their classification and manipulation based on the data we already have. There is no manual intervention, so the process can be repeated every time to the original data. In my experience a large portion of the manual work later on consists of correcting the outcome of the preprocessing phase. It is a very common scenario that we can locate automatically all the cases we are interested about, but there are some false hits. So removing those false cases is often a simple manual task. In some cases it can also be automatized, but it often demands very specific knowledge and human judgement, so in some situations the automatization may not even be desiderable. Some of the most basic preprocessing tools are stringr library and if_else function from dplyr. In order to access adjacent rows we can use functions lag and lead, and combining the query into multiple if_else statements it is possible to classify all cases we have. 2.4.1 Example of preprocessing workflow While coming up with a regular expression and if_else loop, it makes sense to test it first with a dummy data that you know contains all the cases, or all the cases you can envision, in the real data. Already in this point we can wrap the preprocessing workflow into a function that can be applied in an identical manner to the dummy data and the real data. This kind of file can be easily edited in LibreOffice. example 2.5 Analysis workflows There are numerous ways to arrange the workflows around research. It is also important to realize it may not be possible to arrange everything perfectly. One reason is that at times the software we use just doesn’t allow the structure that would be ideal. Or it is allowed, but actually using it would be too slow and impractical. This depends entirely from the question, but it is something good to keep in mind. There are situations where the corpora we use already contain all the annotations we need. Everything. But this is often not the case, and it is necessary to make some new annotations, or modify the existing ones to mark some distinctions which are not currently there. However, it is worth noting that for quite tasks and especially corpus inspection we can just read the corpus in as it is. These new annotations in all cases create a derivative work from the earlier corpus. This makes it important to consider the licenses of the corpora being used. It also rises the question whether the new annotations should be integrated into the corpus itself. One possible workflow is diagrammed below. It includes creating a CSV export with R or Python, manipulating that manually, then reading that into analytical environment for further work. In this scenario the new annotations are made in CSV, and are thus eventually disconnected from the ELAN corpus. It is important to understand that they are disconnected only after the original ELAN corpus is modified in one way or another. If no changes occur, and the spreadsheet contains some identifiers for each processed unit, then in principle the situation is entirely unproblematic. The drawback of this method is that we lose easy connection to the interactive environment in ELAN. Most important part of that being the possibility to listen audio files easily on utterance level. The workflow below describes a method where the annotations are written back to ELAN, either through a spreadsheet or directly from the programming language used for preprocessing. On the other hand, ELAN doesn’t offer a very smooth environment to annotate anything below the word-level, so especially in cases where where we want to work with phonetics it can be more desiderable to write the interesting units into Praat’s TextGrid files. The advantage of this is that we can use PraatScript to do different kind of analysis to the annotated units. 2.6 When to write back to ELAN file In one of the workflow scenarios I discussed the possibility of writing the automatically or manually processed data back to ELAN. There are roughly two situations where this is needed. The annotations are of types that benefit from being done in ELAN The annotations are intended to be integrated to the corpus itself The second option is something that rises several new issues. The first of these is whether the modified new corpus is intended as a direct new version of the original one, or whether it is a derivative work that would be distributed separatedly from the original. It is worth noticing that currently there are very few examples of new data being integrated into corpora this way, although there are many ways this could be beneficial. The new annotations add new information, and further research could be conducted by someone else by continuing from this work that is already done. Problem with adding the new annotations into corpus itself is that there is a question of maintenance responsibility. The new annotations depend from the annotations above them, and if those are modified, it is very easy to end up with a situation where the annotations are not matching with one another any longer, which can have disastrous results for people who try to use the corpus later. As far as I see, new annotations should be integrated to the corpus mainly in cases where they somehow connect to the larger annotation plans of the current corpus curators. There are also license issues to consider. If the corpus you are using is licensed with ND-clause, this effectively bans further derivative works. New annotation layers certainly count as this, so it may be complicated to share this kind of work with other researchers. On the other hand, many of the corpora we are using are so large, that generally speaking their sharing and distribution is a complex issue of its own. 2.7 Annotations as an independent dataset It is also possible to distribute the new annotation layers as an independent dataset. This would demand knowledge of following pieces of information: Version number and source of the corpus used ID’s of the annotations referred to There are cases where different annotation layers have different licenses, which leads into situations as with the NYUAD Arabic UD treebank (https://github.com/UniversalDependencies/UD_Arabic-NYUAD) (Example taken with this query: Imgur The annotations and corpus are stored separatedly, but there is a script provided that allows merging these back together. Although this may look a bit complicated, I think this model will become more popular in the future, as so many datasets are made accessible with unclear licenses. This is unfortunate as the situation could be avoided with clearer licensing practices to start with, but on the other hand it forces to develop more consistent practices with linked datasets, of which the corpus and distinct annotation layer are one example. "],
["tools.html", "Section 3 Tools 3.1 Git 3.2 R 3.3 Python 3.4 Anaconda 3.5 reticulate 3.6 PraatScript 3.7 XPath", " Section 3 Tools This course has in its name both R and Python. I understand this can be criticized, as it is often mentioned that focusing into one or another would be “the best choice” in the long run. I think this thinking fails to understand the actual landscape these programming languages are located in, at least from the perspective of a linguist. I think many discussions about which programming language to focus into comes also from the perspective of professional programmers, and it sounds very plausible that longer and more concentrated work with one language eventually pays off in grandeous mastering of that one. However, for many of us the primary goal may be to get something to work. One solution to this is R package reticulate, which allows using Python from within an R session. First thing to notice is that this ecosystem is on the move. The programming languages themselves are rather stable (well, R really is not), but there are continuously new packages and workflows that can be adapted into our uses. If something already exists in one language, but not in another, I think it is usually easiest to use the already finished and tested implementation. This is especially the case with more complex tasks which have a large amount of corner cases and questions that aren’t obvious in the beginning. In this vein, none of the exact methods in this course are meant as something that will be forever applicable as such, but especially in several years many things get outdated and there will be better and more elegant methods available. However, I think the basic ideas should be valid also in the longer run. There are also tools such as packrat for R and Anaconda or virtualenv for Python, which allow storing exact information about the environment where the code was run. Of course there are also available paths which are not at all touched here. For example, ELAN is written in Java, and the source code is available. It could be very useful to hack into that and extract some of the methods as their own more independent command line tools. ELAN code looks very nice and well written, so manipulating the tool directly should also be a real option and not very difficult for someone who knows Java well. Binding some of the native Java methods into R or Python functions could be a very safe way to manipulate ELAN files, as there would be no difference between this and GUI output. So my main idea here is that just use all tools that are available and which you bother to learn, and if there is something that gets too complicated, just hire someone who knows it to do it fast and effectively. But most importantly, if you do the latter, pay lots of attention to communication so that you all know what you want to be able to do. As far as I see, R and Python are both quite simple and easy to learn as interner is so full of resources, but of course both need more attention to be learned solidly. This, on the other hand, comes easiest after trying to build something you currently need. Maybe the most difficult part in programming nowadays is to figure out the ways how to call the things you want to do. Most of the problems we have are already solved, we just have to find the examples that can be adapted into our needs. In the beginning there are many difficulties, but this comes often from uncertainty of how to call what you want to do. 3.1 Git One of the most important tools we can use is a version control system. There are many alternatives, but at the moment Git can maybe be considered as one of the most supported and accessible systems. This is not to say that Git would be always very straightforward, but especially when we have scenarios where an individual researcher is working with one dataset, the things tend be quite simple. The biggest advantage of version control is that immediately when we get a corpus, we can put all files in it under monitoring that ensures they don’t overgo changes. There is always a possibility that we accidentally change something we didn’t intend, and this may even change our results in the longer run. One additional reason to use version control is that it helps you to keep track of in which order you did or tried things. Especially when writing code for more complicated study, you often end up moving different code blocks up and down when you understand in which order they must be executed. 3.2 R The R NLP ecosystem is now changing very fast. One of the most interesting new developments is tidytext, which is an R package that allows working with text data in so called tidy R framework. This package contains very good function for tokenization, unnest_tokens(), and generally speaking it is worth looking into. The package authors have also written a book, Text mining with R. From ELAN perspective, the most useful package is certainly xml2. There is also an older package XML, but it is rather difficult to use in the end. 3.2.1 tidytext 3.2.2 xml2 3.3 Python Python package Pympi is certainly the most advanced tool currently to manipulate ELAN files programmatically. I think it touches well one of the most basic problems of interacting with the ELAN files: creating new tiers gets very difficult and dangerous. 3.3.1 pympi Pympi can be installed with: pip3 install pympi-ling As a warning I must mention that I think Pympi doesn’t in all cases follow the original ELAN specifications, and the files created with it differ slightly from the ones created when same is done through ELAN GUI. This can be fine, and I don’t think there are parts that do not work, but in some sense it is always good to keep this in mind. We do not know if non-standard structures in some places are always accepted by all ELAN versions. 3.4 Anaconda conda create --name adv_elan source activate adv_elan pip install pympi-ling 3.5 reticulate There is an R package reticulate which allows accessing Python from R. This can be very useful if you don’t want to switch your working environment all the time, although of course setting up things like these doesn’t necessarily make anything easier. But it is an interesting alternative. 3.6 PraatScript When we work with extracting information from the audio files, the situation is commonly that Praat can do big part of the analysis already, and if it can be done in Praat, it can be automatized with PraatScript. This can be executed from R or Python as well. There are also R and Python packages for interacting with Praat, but as far as I see, these are also usually bound to PraatScript in the end, and using them tends to result in complex mixture of the programming language and PraatScript. This surely works when you know both of those well, and I will also study these packages further, but for now I have found it cleaner to keep these two separated. There are few reasons: It is easier to find help for PraatScript or the programming language alone This way PraatScript is easier to reuse for people who just want to deal with PraatScript This said, I don’t really know very much about these packages, so if someone has good experiences, please let me know! I guess my main point is that PraatScript is really useful, and if you do something repeatedly in Praat, please check it and see if it can be adapted into your use. One of the best introductions to the topic is here. 3.7 XPath Although XML is usually considered a human readable file format, I personally would advice against modifying XML directly in the text editor unless there is a specific case what you know you want to do there. The most advanced tool to work with XML files is XSLT. Unfortunately XSLT is very difficult to use and learn. However, there is one part of these core XML technologies which we inevitably need: XPath. It is a small language of its own that can be used to select parts of XML file. It also has a number of functions of its own. 3.7.1 Examples //node = select any node with this name //node/@attribute = select an attribute //node[@attribute='something'] = select node which has an attribute with value //node[@attribute='starts-with(someth)'] = select node which has an attribute which starts with ./child/child = move down to child ./../.. = move up to parent In xml2 R package the XPath expression goes into R function xml_find_all(). With function xml_text() we can retrieve the text of currently selected nodes. suppressPackageStartupMessages(library(tidyverse)) library(xml2) read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;wordT&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;) ## {xml_nodeset (3)} ## [1] &lt;ANNOTATION_VALUE&gt;Words&lt;/ANNOTATION_VALUE&gt; ## [2] &lt;ANNOTATION_VALUE&gt;here&lt;/ANNOTATION_VALUE&gt; ## [3] &lt;ANNOTATION_VALUE&gt;.&lt;/ANNOTATION_VALUE&gt; read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;wordT&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;) %&gt;% xml_text() ## [1] &quot;Words&quot; &quot;here&quot; &quot;.&quot; It is important to understand that xml_find_all() function selects the nodes, but all adjacent nodes are still present in the tree. This is demonstrated below: read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;wordT&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;) %&gt;% xml_find_all(&quot;../../..&quot;) %&gt;% xml_attr(&#39;PARTICIPANT&#39;) ## [1] &quot;Niko&quot; So although we have selected something, we can still access all other content in the tree. "],
["elan-file-structure.html", "Section 4 ELAN file structure 4.1 Minimal file 4.2 Tier type naming convention 4.3 Hierarchies 4.4 Discussion", " Section 4 ELAN file structure In this part we go into deeper details of ELAN XML structure. It is important to understand how ELAN stores information into different tiers, so that we can easily extract the parts we need. It is also very good test for data integrity to parse all the data from ELAN file, since this verifies that all data is stored in a retrievable manner. 4.1 Minimal file &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;ANNOTATION_DOCUMENT AUTHOR=&quot;&quot; DATE=&quot;2017-09-05T14:53:24+06:00&quot; FORMAT=&quot;3.0&quot; VERSION=&quot;3.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:noNamespaceSchemaLocation=&quot;http://www.mpi.nl/tools/elan/EAFv3.0.xsd&quot;&gt; &lt;HEADER MEDIA_FILE=&quot;&quot; TIME_UNITS=&quot;milliseconds&quot;&gt; &lt;MEDIA_DESCRIPTOR MEDIA_URL=&quot;file:///Users/niko/audio.wav&quot; MIME_TYPE=&quot;audio/x-wav&quot; RELATIVE_MEDIA_URL=&quot;../../audio.wav&quot;/&gt; &lt;PROPERTY NAME=&quot;URN&quot;&gt;urn:nl-mpi-tools-elan-eaf:7e5bf3cc-a72f-4868-810b-2ca34876e64c&lt;/PROPERTY&gt; &lt;PROPERTY NAME=&quot;lastUsedAnnotationId&quot;&gt;5&lt;/PROPERTY&gt; &lt;/HEADER&gt; &lt;TIME_ORDER&gt; &lt;TIME_SLOT TIME_SLOT_ID=&quot;ts1&quot; TIME_VALUE=&quot;0&quot;/&gt; &lt;TIME_SLOT TIME_SLOT_ID=&quot;ts2&quot; TIME_VALUE=&quot;1000&quot;/&gt; &lt;/TIME_ORDER&gt; &lt;TIER LINGUISTIC_TYPE_REF=&quot;refT&quot; PARTICIPANT=&quot;Niko&quot; TIER_ID=&quot;ref@Niko&quot;&gt; &lt;ANNOTATION&gt; &lt;ALIGNABLE_ANNOTATION ANNOTATION_ID=&quot;a1&quot; TIME_SLOT_REF1=&quot;ts1&quot; TIME_SLOT_REF2=&quot;ts2&quot;&gt; &lt;ANNOTATION_VALUE&gt;.001&lt;/ANNOTATION_VALUE&gt; &lt;/ALIGNABLE_ANNOTATION&gt; &lt;/ANNOTATION&gt; &lt;/TIER&gt; &lt;TIER LINGUISTIC_TYPE_REF=&quot;orthT&quot; PARENT_REF=&quot;ref@Niko&quot; PARTICIPANT=&quot;Niko&quot; TIER_ID=&quot;orth@Niko&quot;&gt; &lt;ANNOTATION&gt; &lt;REF_ANNOTATION ANNOTATION_ID=&quot;a2&quot; ANNOTATION_REF=&quot;a1&quot;&gt; &lt;ANNOTATION_VALUE&gt;words here&lt;/ANNOTATION_VALUE&gt; &lt;/REF_ANNOTATION&gt; &lt;/ANNOTATION&gt; &lt;/TIER&gt; &lt;TIER LINGUISTIC_TYPE_REF=&quot;wordT&quot; PARENT_REF=&quot;orth@Niko&quot; PARTICIPANT=&quot;Niko&quot; TIER_ID=&quot;word@Niko&quot;&gt; &lt;ANNOTATION&gt; &lt;REF_ANNOTATION ANNOTATION_ID=&quot;a4&quot; ANNOTATION_REF=&quot;a2&quot;&gt; &lt;ANNOTATION_VALUE&gt;words&lt;/ANNOTATION_VALUE&gt; &lt;/REF_ANNOTATION&gt; &lt;/ANNOTATION&gt; &lt;ANNOTATION&gt; &lt;REF_ANNOTATION ANNOTATION_ID=&quot;a5&quot; ANNOTATION_REF=&quot;a2&quot; PREVIOUS_ANNOTATION=&quot;a4&quot;&gt; &lt;ANNOTATION_VALUE&gt;here&lt;/ANNOTATION_VALUE&gt; &lt;/REF_ANNOTATION&gt; &lt;/ANNOTATION&gt; &lt;/TIER&gt; &lt;LINGUISTIC_TYPE GRAPHIC_REFERENCES=&quot;false&quot; LINGUISTIC_TYPE_ID=&quot;refT&quot; TIME_ALIGNABLE=&quot;true&quot;/&gt; &lt;LINGUISTIC_TYPE CONSTRAINTS=&quot;Symbolic_Association&quot; GRAPHIC_REFERENCES=&quot;false&quot; LINGUISTIC_TYPE_ID=&quot;orthT&quot; TIME_ALIGNABLE=&quot;false&quot;/&gt; &lt;LINGUISTIC_TYPE CONSTRAINTS=&quot;Symbolic_Subdivision&quot; GRAPHIC_REFERENCES=&quot;false&quot; LINGUISTIC_TYPE_ID=&quot;wordT&quot; TIME_ALIGNABLE=&quot;false&quot;/&gt; &lt;CONSTRAINT DESCRIPTION=&quot;Time subdivision of parent annotation&#39;s time interval, no time gaps allowed within this interval&quot; STEREOTYPE=&quot;Time_Subdivision&quot;/&gt; &lt;CONSTRAINT DESCRIPTION=&quot;Symbolic subdivision of a parent annotation. Annotations refering to the same parent are ordered&quot; STEREOTYPE=&quot;Symbolic_Subdivision&quot;/&gt; &lt;CONSTRAINT DESCRIPTION=&quot;1-1 association with a parent annotation&quot; STEREOTYPE=&quot;Symbolic_Association&quot;/&gt; &lt;CONSTRAINT DESCRIPTION=&quot;Time alignable annotations within the parent annotation&#39;s time interval, gaps are allowed&quot; STEREOTYPE=&quot;Included_In&quot;/&gt; &lt;/ANNOTATION_DOCUMENT&gt; This is one file with three tiers. The first is reference tier, containing the ID for the utterance, the second is the transcription, and the last one contains the tokenized words. It is the kind of structure used in language documentation projects connected to Freiburg. Other structures are certainly possible, although some kind of hierarchy between tiers is generally encouraged. At least following guidelines can be distinguished: Each content type should have its own tier type If there is a hierarchical relation, it should be used instead of independent tiers Ideally each speaker has his/her independent group of tiers which have shared structure between speakers 4.1.1 Participant name convention Some parts are more a matter of preferred convention. Each tier contains information about participants in two possible places: PARTICIPANT=&quot;Niko&quot; TIER_ID=&quot;ref@Niko This makes a difference in which convention has to be used in order to select the nodes of this speaker. If we want to know who is the speaker on a tier, we can either pick the attribute PARTICIPANT or the content after @-character in the tier id. 4.2 Tier type naming convention The situation is similar with selecting all tiers of specific type: LINGUISTIC_TYPE_REF=&quot;refT&quot; TIER_ID=&quot;ref@Niko&quot; Also here the tier type is specified in both the type name and as part of the tier ID. So the following commands are in this case identical: suppressPackageStartupMessages(library(tidyverse)) library(xml2) read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;wordT&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;) ## {xml_nodeset (3)} ## [1] &lt;ANNOTATION_VALUE&gt;Words&lt;/ANNOTATION_VALUE&gt; ## [2] &lt;ANNOTATION_VALUE&gt;here&lt;/ANNOTATION_VALUE&gt; ## [3] &lt;ANNOTATION_VALUE&gt;.&lt;/ANNOTATION_VALUE&gt; read_xml(&#39;test.eaf&#39;) %&gt;% xml_find_all(&quot;//TIER[starts-with(@TIER_ID, &#39;word&#39;)]/ANNOTATION/*/ANNOTATION_VALUE&quot;) ## {xml_nodeset (3)} ## [1] &lt;ANNOTATION_VALUE&gt;Words&lt;/ANNOTATION_VALUE&gt; ## [2] &lt;ANNOTATION_VALUE&gt;here&lt;/ANNOTATION_VALUE&gt; ## [3] &lt;ANNOTATION_VALUE&gt;.&lt;/ANNOTATION_VALUE&gt; In the situations where every content type doesn’t have its own type, it may be more useful to select them by part of the tier ID. 4.3 Hierarchies Very often we want to have the time spans of different units, even when we are dealing with symbolically subdivided words that don’t have this information for the units themselves. Only the highermost tier in ELAN has the information about the time codes, so associating lower level tiers with their times usually demands reconstructing the hierarchies in the file. 4.4 Discussion The conventions specified above define how we are supposed to extract the wanted information. Usually this is done with the goal to get all content of all speakers from the tier X. We want all utterances, or all words. However, often we would like to have a file that contains larger amount of data in one structure. "],
["parsing-elan-files-to-r.html", "Section 5 Parsing ELAN files to R 5.1 Customizing to tier pattern 5.2 Why to read ELAN files into R? 5.3 Parsing with FRelan package", " Section 5 Parsing ELAN files to R Let’s assume we have a following kind of ELAN file: Example ELAN file If this would be parsed into R, the expected structure would be something like: example &lt;- tribble(~token, ~form, ~utterance, ~reference, ~participant, ~time_start, ~time_end, ~filename, &#39;words&#39;, &#39;Words&#39;, &#39;Words here.&#39;, &#39;.001&#39;, &#39;Niko&#39;, 0, 1000, &#39;kpv_izva20171010test.eaf&#39;, &#39;here&#39;, &#39;here&#39;, &#39;Words here.&#39;, &#39;.001&#39;, &#39;Niko&#39;, 0, 1000, &#39;kpv_izva20171010test.eaf&#39;, &#39;.&#39;, &#39;.&#39;, &#39;Words here.&#39;, &#39;.001&#39;, &#39;Niko&#39;, 0, 1000, &#39;kpv_izva20171010test.eaf&#39;) %&gt;% mutate(session_name = str_extract(filename, &#39;.+(?=.eaf)&#39;)) example %&gt;% knitr::kable() token form utterance reference participant time_start time_end filename session_name words Words Words here. .001 Niko 0 1000 kpv_izva20171010test.eaf kpv_izva20171010test here here Words here. .001 Niko 0 1000 kpv_izva20171010test.eaf kpv_izva20171010test . . Words here. .001 Niko 0 1000 kpv_izva20171010test.eaf kpv_izva20171010test As far as I see, this is what we have in the ELAN file. Of course there are other pieces of information such as annotator, language of the tier, last editing time, media files and so on, but I have not needed those very much myself. The convention I have had with the media files is that each file has media files named identically with the ELAN file itself. Thereby their names (and paths) can be extracted from the ELAN file names if and when needed. Things can be complicated when there is more content below the tiers. For example, there often are glosses, lemmas and pos-tags below the word tokens. Those will be discussed below. First I want to mention the possibility to combine metadata into the ELAN files. The starting point here is, usually, that we usually have in this point two values we need: participant and session name. We didn’t discuss the session names yet, but in my convention each ELAN file has an unique name, and this also serves as the general session name by which this recording instance can be distinguished from the rest. If we think metadata more generally, we normally have variables which are connected into one of these items. The session itself has a recording place and time, it has files, it has setting, more and genre, for example. The participants themselves have birthtime, age, place of residence, language skills, roles in the recording and so on. It is important to note that some of the variables listed above are quite different from others. Especially age and role are something that make sense only in the combination of the participant and the recording: the age is always different, and the roles can also vary. We have numerous recordings where the interviewer gets to be the interviewee and vice versa. So from this point of view the “result” we actually want to work with often looks more like this: place_meta &lt;- tibble(session_name = c(&#39;kpv_izva20171010test&#39;, &#39;kpv_izva20171015test2&#39;), rec_place = c(&#39;Paris, France&#39;, &#39;Syktyvkar, Komi&#39;), rec_place_lat = c(48.864716, 0), rec_place_lon = c(2.349014, 0)) speaker_meta &lt;- tibble(participant = &#39;Niko&#39;, birthplace = &#39;Sulkava, Finland&#39;, bplace_lat = 61.786100, bplace_lon = 28.370586, birthyear = 1986) corpus &lt;- left_join(example, place_meta) %&gt;% left_join(speaker_meta) ## Joining, by = &quot;session_name&quot; ## Joining, by = &quot;participant&quot; corpus ## # A tibble: 3 x 16 ## token form utterance reference participant time_start time_end ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 words Words Words here. .001 Niko 0 1000 ## 2 here here Words here. .001 Niko 0 1000 ## 3 . . Words here. .001 Niko 0 1000 ## # ... with 9 more variables: filename &lt;chr&gt;, session_name &lt;chr&gt;, ## # rec_place &lt;chr&gt;, rec_place_lat &lt;dbl&gt;, rec_place_lon &lt;dbl&gt;, ## # birthplace &lt;chr&gt;, bplace_lat &lt;dbl&gt;, bplace_lon &lt;dbl&gt;, birthyear &lt;dbl&gt; As we are joining the tables by participant and session_name, we are able to handle the fact that some variables may have different values in different sessions. For example, in this case the session metadata table contained two different sessions from different years, but when the joins were done the non-matching ones were discarded. This is one of the properties of left_join, there are other join types that are also often useful. These examples reflect the naming convention that has been used in Freiburg based research projects. {language}_{variety}{YYYYMMDD}-{Nth-recording} There are few variations of this, but the basic idea is similar. Some aspects of metadata are stored already in the filenames. I have heard regularly arguments that ideally the filenames would contain absolutely no metadata, which probably is good for personal privacy reasons. However, there are also pragmatic reasons to have filenames contain something that makes them easy or possible to navigate by humans. In the same sense it can be useful to have there some mnemonic element, which we have also at times added into the end, but this alos adds some new problems. However, having some pieces of information in the filenames allows us to get few more metadata columns in this point: corpus &lt;- dir(&#39;corpus/&#39;, pattern = &#39;eaf$&#39;, full.names = TRUE) %&gt;% map(read_eaf) %&gt;% bind_rows() # We assume the session name starts with a three letter ISO code # If the system is entirely reliable, we can just take three first characters corpus &lt;- corpus %&gt;% mutate(lang = str_extract(session_name, &#39;.{3}&#39;)) %&gt;% mutate(variety = str_extract(session_name, &#39;(?&lt;=.{3}_)[a-z]+(?=\\\\d)&#39;)) %&gt;% mutate(rec_year = as.numeric(str_extract(session_name, &#39;\\\\d{4}(?=\\\\d{4})&#39;))) %&gt;% mutate(gender = str_extract(participant, &#39;(?&lt;=-)(F|M)(?=-)&#39;)) corpus %&gt;% select(lang, variety, rec_year, gender, everything()) ## # A tibble: 568 x 15 ## lang variety rec_year gender token ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 kpv izva 2014 F ме ## 2 kpv izva 2014 F , ## 3 kpv izva 2014 F кӧнечнэ ## 4 kpv izva 2014 F же ## 5 kpv izva 2014 F , ## 6 kpv izva 2014 F кык ## 7 kpv izva 2014 F лун ## 8 kpv izva 2014 F вӧлі ## 9 kpv izva 2014 F в ## 10 kpv izva 2014 F шоке ## # ... with 558 more rows, and 10 more variables: utterance &lt;chr&gt;, ## # reference &lt;chr&gt;, participant &lt;chr&gt;, time_start &lt;dbl&gt;, time_end &lt;dbl&gt;, ## # session_name &lt;chr&gt;, filename &lt;chr&gt;, word &lt;chr&gt;, after &lt;chr&gt;, ## # before &lt;chr&gt; corpus %&gt;% count(variety) ## # A tibble: 2 x 2 ## variety n ## &lt;chr&gt; &lt;int&gt; ## 1 izva 348 ## 2 udo 220 corpus %&gt;% count(gender) ## # A tibble: 2 x 2 ## gender n ## &lt;chr&gt; &lt;int&gt; ## 1 F 281 ## 2 M 287 corpus %&gt;% count(participant) ## # A tibble: 4 x 2 ## participant n ## &lt;chr&gt; &lt;int&gt; ## 1 JAI-M-1939 246 ## 2 JSS-F-1988 183 ## 3 MVF-F-1984 98 ## 4 NTP-M-1986 41 This is one place where standardized and cross-comparable metadata would be useful. It seems to me that lots of the metadata conversation has circulated around the archiving and later data foundability needs, but this questions come to us also on very concrete levels. How do we call the columns in an R dataframe? You have to refer to them all the time, so changing it later will break lots of code that worked earlier. I guess having something systematic that works for you is the best solution for now. Probably the naming conventions of different programming languages are also good to be taken into account. The idea behind the structure in this data frame is that we can consider each token as one observation, and this format with one token per row allows easy plotting and statistical testing without issues. The main issue of this section is that we ultimately have to take quite much time to think from where different pieces of metadata come from, or whether they can or cannot be derived from the data we have at hand. 5.0.1 Questions Places, coordinates? How exact times we have and need? Are there some pieces of metadata we need all the time? What kind of data tends to change? 5.0.2 Example I have written in this blog about more exact conventions in parsing ELAN files, especially within FRelan R package that I have built to work with Freiburg project ELAN files. However, I have not been very succesful in getting some system to work so that it would be easily and intuitively adapted into project with an entirely different tier structure. Probably the easiest way is to modify the code every time there is a new tier structure. 5.1 Customizing to tier pattern One of the example files has structure: |-ref (id) \\- orth (kpv) \\- word (tokenized from orth) \\- lemma \\- pos \\- ft-rus (Russian translation) \\- ft-eng (English translation) \\- note (different notes on utterance level) The way I usually approach this is to read into R individual tiers, and join them together following the logic by which the ELAN tier structure has been set up. read_custom_eaf &lt;- function(path_to_file){ ref &lt;- FRelan::read_tier(eaf_file = path_to_file, linguistic_type = &quot;refT&quot;) %&gt;% dplyr::select(content, annot_id, participant, time_slot_1, time_slot_2) %&gt;% dplyr::rename(ref = content) %&gt;% dplyr::rename(ref_id = annot_id) orth &lt;- FRelan::read_tier(eaf_file = path_to_file, linguistic_type = &quot;orthT&quot;) %&gt;% dplyr::select(content, annot_id, ref_id, participant) %&gt;% dplyr::rename(orth = content) %&gt;% dplyr::rename(orth_id = annot_id) token &lt;- FRelan::read_tier(eaf_file = path_to_file, linguistic_type = &quot;wordT&quot;) %&gt;% dplyr::select(content, annot_id, ref_id, participant) %&gt;% dplyr::rename(token = content) %&gt;% dplyr::rename(token_id = annot_id) %&gt;% dplyr::rename(orth_id = ref_id) lemma &lt;- FRelan::read_tier(eaf_file = path_to_file, linguistic_type = &quot;lemmaT&quot;) %&gt;% dplyr::select(content, annot_id, ref_id, participant) %&gt;% dplyr::rename(lemma = content) %&gt;% dplyr::rename(lemma_id = annot_id) %&gt;% dplyr::rename(token_id = ref_id) pos &lt;- FRelan::read_tier(eaf_file = path_to_file, linguistic_type = &quot;posT&quot;) %&gt;% dplyr::select(content, ref_id, participant) %&gt;% dplyr::rename(pos = content) %&gt;% dplyr::rename(lemma_id = ref_id) elan &lt;- left_join(ref, orth) %&gt;% left_join(token) %&gt;% left_join(lemma) %&gt;% left_join(pos) %&gt;% select(token, lemma, pos, time_slot_1, time_slot_2, everything(), -ends_with(&#39;_id&#39;)) time_slots &lt;- FRelan::read_timeslots(path_to_file) elan %&gt;% left_join(time_slots %&gt;% rename(time_slot_1 = time_slot_id)) %&gt;% rename(time_start = time_value) %&gt;% left_join(time_slots %&gt;% rename(time_slot_2 = time_slot_id)) %&gt;% rename(time_end = time_value) %&gt;% select(token, lemma, pos, participant, time_start, time_end, everything(), -starts_with(&#39;time_slot_&#39;)) } read_custom_eaf(&#39;corpus/kpv_udo20120330SazinaJS-encounter.eaf&#39;) ## Joining, by = c(&quot;ref_id&quot;, &quot;participant&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;orth_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;token_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;lemma_id&quot;) ## Joining, by = &quot;time_slot_1&quot; ## Joining, by = &quot;time_slot_2&quot; ## # A tibble: 239 x 8 ## token lemma pos participant time_start time_end ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 И и CC NTP-M-1986 170 3730 ## 2 эшшӧ эшшӧ _ NTP-M-1986 170 3730 ## 3 ӧтик ӧтик Num NTP-M-1986 170 3730 ## 4 тор тор N NTP-M-1986 170 3730 ## 5 , , CLB NTP-M-1986 170 3730 ## 6 мый мый CS NTP-M-1986 170 3730 ## 7 тэнад тэ Pron NTP-M-1986 170 3730 ## 8 , , CLB NTP-M-1986 170 3730 ## 9 тэныд тэ Pron NTP-M-1986 170 3730 ## 10 мам мам N NTP-M-1986 170 3730 ## # ... with 229 more rows, and 2 more variables: ref &lt;chr&gt;, orth &lt;chr&gt; read_custom_eaf(&#39;corpus/kpv_izva20140330-1-fragment.eaf&#39;) ## Joining, by = c(&quot;ref_id&quot;, &quot;participant&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;orth_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;token_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;lemma_id&quot;) ## Joining, by = &quot;time_slot_1&quot; ## Joining, by = &quot;time_slot_2&quot; ## # A tibble: 98 x 8 ## token lemma pos participant time_start time_end ## &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ме NA NA MVF-F-1984 0 6086 ## 2 , NA NA MVF-F-1984 0 6086 ## 3 кӧнечнэ NA NA MVF-F-1984 0 6086 ## 4 же NA NA MVF-F-1984 0 6086 ## 5 , NA NA MVF-F-1984 0 6086 ## 6 кык NA NA MVF-F-1984 0 6086 ## 7 лун NA NA MVF-F-1984 0 6086 ## 8 вӧлі NA NA MVF-F-1984 0 6086 ## 9 в NA NA MVF-F-1984 0 6086 ## 10 шоке NA NA MVF-F-1984 0 6086 ## # ... with 88 more rows, and 2 more variables: ref &lt;chr&gt;, orth &lt;chr&gt; In practice the files would be parsed in following manner: corpus &lt;- dir(&#39;corpus&#39;, pattern = &#39;eaf$&#39;, full.names = TRUE) %&gt;% map(read_custom_eaf) %&gt;% bind_rows() ## Joining, by = c(&quot;ref_id&quot;, &quot;participant&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;orth_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;token_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;lemma_id&quot;) ## Joining, by = &quot;time_slot_1&quot; ## Joining, by = &quot;time_slot_2&quot; ## Joining, by = c(&quot;ref_id&quot;, &quot;participant&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;orth_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;token_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;lemma_id&quot;) ## Joining, by = &quot;time_slot_1&quot; ## Joining, by = &quot;time_slot_2&quot; ## Joining, by = c(&quot;ref_id&quot;, &quot;participant&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;orth_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;token_id&quot;) ## Joining, by = c(&quot;participant&quot;, &quot;lemma_id&quot;) ## Joining, by = &quot;time_slot_1&quot; ## Joining, by = &quot;time_slot_2&quot; 5.2 Why to read ELAN files into R? I have often seen and read the idea that in order to analyze linguistic data in R the first task should be to export data from ELAN into a spreadsheet. The problems with this approachs are manifold: The spreadsheet has to be manually updated every time the ELAN file changes If the spreadsheet is annotated further, it will not match with the updates done into ELAN files after original export From this point of view the ideal solution would be to store all information into ELAN file and create an automatic export procedure to analyse the data further, or store it in other format such as spreadsheet in case this is needed for some reason. Only way to keep this working is through meticulous working practices. There is no way over this. However, these practices can be easened and made more robust by adopting workflows which minimize the possibilities to deviate from the convention. The most simple mechanism for this is to have a system that is as minimal as possible, still allowing the tasks we want to achieve. Unfortunately the situations and the tools we work with are never perfect, and there may be situations where we settle into a workflow far from ideal, due to different constraints: time, practicality, limitations of software. In these cases it is important to notice where these compromises have been made. I have included into each example presented here also the points which will make this procedure difficult to maintain, if these can be easily imagined. 5.3 Parsing with FRelan package The function I have in the FRelan package for parsing one tier looks like this: FRelan::read_tier ## function(eaf_file = &quot;/Volumes/langdoc/langs/kpv/kpv_izva20140404IgusevJA/kpv_izva20140404IgusevJA.eaf&quot;, linguistic_type = &quot;wordT&quot;, read_file = T, xml_object = F){ ## ## `%&gt;%` &lt;- dplyr::`%&gt;%` ## ## if (read_file == F){ ## ## file = xml_object ## ## } else { ## ## file &lt;- xml2::read_xml(eaf_file) ## ## } ## ## participants_in_file &lt;- file %&gt;% xml2::xml_find_all(paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39;]&quot;)) %&gt;% ## xml2::xml_attr(&quot;PARTICIPANT&quot;) ## ## coerce_data_frame &lt;- function(current_participant){ ## dplyr::data_frame( ## content = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, current_participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE&quot;)) %&gt;% ## xml2::xml_text(), ## annot_id = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, current_participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/..&quot;)) %&gt;% ## xml2::xml_attr(&quot;ANNOTATION_ID&quot;), ## ref_id = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, current_participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/..&quot;)) %&gt;% ## xml2::xml_attr(&quot;ANNOTATION_REF&quot;), ## speaker = current_participant, ## tier_id = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, current_participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/../../..&quot;)) %&gt;% ## xml2::xml_attr(&quot;TIER_ID&quot;), ## type = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, current_participant,&quot;&#39;]/ANNOTATION/*/ANNOTATION_VALUE/../../..&quot;)) %&gt;% ## xml2::xml_attr(&quot;LINGUISTIC_TYPE_REF&quot;), ## time_slot_1 = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, current_participant,&quot;&#39;]/ANNOTATION/*&quot;)) %&gt;% ## xml2::xml_attr(&quot;TIME_SLOT_REF1&quot;), ## time_slot_2 = file %&gt;% ## xml2::xml_find_all( ## paste0(&quot;//TIER[@LINGUISTIC_TYPE_REF=&#39;&quot;, linguistic_type, &quot;&#39; and @PARTICIPANT=&#39;&quot;, current_participant,&quot;&#39;]/ANNOTATION/*&quot;)) %&gt;% ## xml2::xml_attr(&quot;TIME_SLOT_REF2&quot;)) ## } ## ## if (length(participants_in_file) != 0){ ## ## plyr::ldply(participants_in_file, coerce_data_frame) %&gt;% dplyr::tbl_df() %&gt;% dplyr::rename(participant = speaker) ## ## } else { ## ## all_participants &lt;- file %&gt;% xml2::xml_find_all(&quot;//TIER&quot;) %&gt;% ## xml2::xml_attr(&quot;PARTICIPANT&quot;) %&gt;% unique() ## tibble(content = NA, ## annot_id = &#39;&#39;, ## ref_id = &#39;&#39;, ## participant = all_participants, ## tier_id = &#39;&#39;, ## type = linguistic_type, ## time_slot_1 = NA, ## time_slot_2 = NA) ## ## } ## ## ## } ## &lt;bytecode: 0x7fabc6558ad8&gt; ## &lt;environment: namespace:FRelan&gt; It works fine, but I have not yet updated it to use map() function from purrr package. It also has some extra parts in it to parse either an XML file or an XML file already read into memory. We usually want to read several tiers and merge them together, and in this context it is not good to open the XML file again every time we read it, as this would slow the function down very much. "],
["exploring-and-manipulation.html", "Section 6 Exploring and manipulation 6.1 Basic exploration with R 6.2 Manipulating ELAN files with Pympi", " Section 6 Exploring and manipulation 6.1 Basic exploration with R I go through in this point some of the most important dplyr verbs that can be used to examine data from ELAN files. There are more thorough introduction in the internet, so this treatment is not very complete view. 6.1.1 filter() 6.1.2 slice() 6.1.3 count(), add_count() 6.1.4 mutate 6.1.5 rename 6.1.6 lag(), lead() These functions can be used to access rows above or below another token 6.1.7 str_extract() 6.1.8 str_detect() 6.1.9 if_else() 6.2 Manipulating ELAN files with Pympi Pympi is a very mature Python library and using it is enjoyable. However, I myself usually feel the most comfortable when I have the data in an R data frame, so I often fall back into that no matter what. I strongly encourage to test further data manipulation and analysis in Python, I assume that especially pandas should be very useful. "],
["pympi-examples.html", "Section 7 Pympi examples 7.1 Creating a new ELAN file 7.2 Populating the ELAN file with content 7.3 Merging ELAN files", " Section 7 Pympi examples 7.1 Creating a new ELAN file 7.2 Populating the ELAN file with content 7.3 Merging ELAN files "],
["shiny-components.html", "Section 8 Shiny components 8.1 DT 8.2 Leaflet 8.3 ggplot2 8.4 Advantages and disadvantages of Shiny", " Section 8 Shiny components The easiest way to build a new Shiny application is to find the pieces you need from Shiny Gallery and start adjusting from this. Shiny applications and their functions differ from normal R functions in what are called reactive functions. These functions are constructed so that they can get their arguments interactively from the UI part of Shiny code. Grasping how the reactive functions work is little bit challenging, but once you get touch to it the basic concept will not change. The Shiny application has two parts. They are called ui and server. The code is executed on the server side, and the variables that the user manipulates are on UI side. The results computed on the server code will again be placed as output functions to the UI, which creates clear interaction between these parts. It is possible to have the Shiny app in two different files called ui.R and server.R, but they can also be in one function. With more complicated applications it is often easier to put them into different files, but there are also situations where compactness of one file can make it easier to follow. I often do the first prototype into one function and split it later when I start adding bells and whistles. In this point it is good to mention, that there is a specific set of HTML Widgets that interact very well with Shiny. They are different from Shiny applications as they are just plain HTML and JavaScript and can be opened in any browser, and they also can be interactive, but the interactivity is usually somewhat limited. Shiny can be used to tie easily two distinct processes into one another. There are new HTML Widgets coming all the time, and apparently creating new ones is not too difficult. The idea in an HTML Widget is that an R object is transformed into HTML which contains JavaScript. Thereby it is not necessary to write JavaScript ourselves, but we get to benefit from large variety of already well working and pretty visualizations. These are called JavaScript library bindings. The concept exists also in Python. This comes with a caveout that if something in the JavaScript library is not yet implemented in the binding R function, it can be quite tricky to get it work the intended way without touching more into JavaScript. JavaScript is nice language, and there is a good introduction JavaScript for Cats, but this can still be something that takes unexpectedly much time. So if your supervisor or colleague sees your HTML Widget or Shiny application, they may suggest different kinds of additions. Do not say yes before googling a bit. The situation is paradoxical in the sense that very impressive presentations can be created with few lines of code, but a small adjustment can be very difficult to set up. Next I will present the HTML Widgets which in my opinion have been the most useful while working with linguistic data. They are also used in later examples. 8.1 DT datatables is a JavaScript library that can be used to make very nice looking tables. In principle a KWIC style concordance is also just a table with specific formatting, as seen in Example… 8.2 Leaflet This is the best package for creating interactive maps at the moment. This is demonstrated in Example… 8.3 ggplot2 This is kind of an old work horse, it is probably the “basic” plotting library in R nowadays, and having familiarity with this will come handy later. It is also easy to make it interact with Shiny, as seen in Example… 8.4 Advantages and disadvantages of Shiny If the concept you have in mind is simple enough to be realized as an HTML Widget, then that is a better way to go. Those can be hosted online anywhere, for example in GitHub, and they work very reliably on any browser. The trick here is that HTML Widget executes only JavaScript in the end, so using it doesn’t differ from any other website which has some JavaScript based visualizations. With Shiny applications the situation is more complicated. They can be ran locally, and then the application can also easily access all local files. They can also be ran on server, but this needs bit more work as it doesn’t seem to be so trivial to set up one, although there are good instructions online. The most common way to host a Shiny application online is probably through RStudio owned website shinyapps.io, and this is usually a very sufficient alternative. On the free account there are limits into how much the application can be used in a month, but for small number of users this shouldn’t be any problem. Of course this all leaves a nagging feeling, as it is always possible to come up with something that has more vivid number of users, and I’m rather sure in this case the free account would run out of allocated minutes very fast. One good midway is to host the application in GitHub, and instruct the user to run it directly from there on their own computer with a function shiny::runGitHub. This is not the same as having it available online for everybody, but it is one way which I have preferred. To be clear, the reason why the Shiny application has to be hosted on the server is that there R code being executed while you use it, and this is something web browser doesn’t handle. I assume the functionality of any Shiny application could be replicated in JavaScript, and then it would work in browser as such, but I can’t even imagine how many lines of code that would eventually demand. This leads to the one of the key aspects of Shiny: rapid prototyping time. Once you know the basic logic of the app, it is possible to implement entirely new kind of interface in a matter of hours. If we spend some time in standardizing our data, so that we all use same concepts when we speak refer to metadata and different parts of our annotations, I could imagine many parts of code can be freely reused as such. "],
["example-interaction-with-emur.html", "Section 9 Example: Interaction with emuR 9.1 Procedure", " Section 9 Example: Interaction with emuR This is a bit specific example and basically just copies the example code from emuR package documentation. This R package connects into tools set up in the University of Munchen. What we are doing here is so called forced alignation. This means automatic matching of text and speech signal. The problem here is that I don’t consider forced alignation to be satisfactorily solved for our use at the moment, not in the way the method is normally used. The tools normally assume that there is 100% correspondence between the transcribed text and audio, and try to align that as exactly as possible. This works well, but this kind of correspondence is much rarer than we would hope for. It is very common that transcriptions have been edited and the original audio contains only some parts of the text, or have segments which are missing from the transcriptions. I haven’t found yet a tool that could do forced alignation well enough with the transcriptions and recordings I have at hand from archives and old publications. The situation is different when we adjust the scale, on small segments the performance is better. And the way forced alignation is done is often such that we get lots of extra information: word and phoneme boundaries. This way forced alignation can well be used with the file typically contained in ELAN files. 9.1 Procedure In this example we use a script that cuts each utterance in the ELAN file into a new WAV-file, and saves the content of the transcription as a new text file. These two files will be matched by their names. This gives us small snippets of WAV files, which still correspond exactly with what we have in the ELAN file. There are inherent problems in taking data out from ELAN this way. What if we decide to change the transcription in original ELAN file? We are going to derive a bunch of new files, and do we also have to keep those synchronized somehow? The main question is how we make sure that we know into what data we are referring to later on. I leave this question unanswered now, but I have to emphasize that this is something we can’t avoid thinking. So the steps we need to do are: Find all utterances Split file by those boundaries and save the results Save the utterance text files Send those to Munchen with emuR Manipulate the emuR result until you have nice Praat files After this it is necessary to fix the segmentation, as it is certainly not perfect. It can be pretty good with a language officially supported by MAUS tools, but with languages like Komi we have to do all kinds of ugly hacks to get it work. Anyway the amount of work is reduced significantly, and we can focus into the most important part of it: deciding with our expert knowledge where the correct phoneme boundaries are. "],
["example-interaction-with-praat.html", "Section 10 Example: Interaction with Praat 10.1 Research questions 10.2 Implementation 10.3 Shiny application 10.4 Observations 10.5 Exercise", " Section 10 Example: Interaction with Praat This example needs data in Praat files with annotation on phoneme level. This is very time consuming to create, but as we saw in Example 1, we can get almost readily usable data through already existing tools using the forced alignment methods offered by Munchen University. So the data we have in hand looks like this: 10.1 Research questions Out of the annotated vowels, which are correctly analysed and which are somehow wrong? How do the vowel formants plot Can we very easily identify the wrongly analysed ones and correct those? 10.2 Implementation We are using several smaller tools here: sendpraat to communicate with Praat PraatScript to extract the formants and write .formant files R to read those files and reshape those into a nice dataframe R to plot the output Shiny R package to wrap this into a shallow interface 10.3 Shiny application The application looks like below. It has only a few functionalities: Select the vowel by clicking it See what is selected under the plot Open the current selection in Praat Play the word where the currently selected vowel is Filter by length (not really useful in this case, but worth checking) Some of these are compromises in a way. It is easy to set up the file to open in Praat, but this of course works only locally. It is not possible to deploy an app like this online. This leads into new questions. How much effort should we pay to make the tool maximally reusable? The question is not trivial, as setting up it more complexly would easily lead into tens of hours spent with adjusting things. In some cases it is useful just to say that this works for me, this is how I use it, this is enough for now. Put the code into GitHub and hope someone else fixes the harder problems. 10.4 Observations There are vowel reductions in faster speech which are not very well described in Komi It seems that the Russian and Komi vowels plot differently, do they? 10.5 Exercise How would you test if the Russian and Komi vowels plot differently? Hints: Skip for now the linguistic part of the question, just think how you get this information into the program. Also skip the statistical part of how you actually verify what you have, this is very simple exercise which is supposed to illustrate moving data in and out. "],
["example-concordances-and-map.html", "Section 11 Example: Concordances and map 11.1 Use", " Section 11 Example: Concordances and map This example is very simple and involves some manipulation of data table object created with DT package. Idea is to create a simple search interface, which can be used conveniently in the end of longer command with the pipe operator. Since DT package outputs normal JavaScript object, there are numerous ways to manipulate it with JavaScript besides R. I’m not going into this now. This application also illustrates some ways to add new information into different tabs. Because the object displayed in the table is just a data frame that has gone through the filtering specified in the boxes above, it is also possible to visualize data that is part of the same data frame, even when it is not present in that table. So in this case all the data comes also with coordinates, which are mapped with the package leaflet. 11.1 Use corpus %&gt;% filter(gender == &#39;M&#39;) %&gt;% FRorpus() "],
["example-preprocessing-workflow.html", "Section 12 Example: Preprocessing workflow 12.1 From points to polygon", " Section 12 Example: Preprocessing workflow In this example I walk through a preliminary exploration of one morphological variable in Komi-Zyrian dialects. I use general dplyr and purrr functions described in section Tools: R. In principle there are four patterns that occur regularly: мунісныс, мунісны, муніны, муніныс. Only two first occur in this data, but the regex would in principle capture all of them. The idea is to define a general function that with the simplest possible example data finds and classifies the wanted examples. This way we can differentiate the query and processing from the data we apply it into. So in principle we could take another Komi dialect sample and run similar analysis into that. dummy_data &lt;- tibble(token = c(&#39;мунісныс&#39;, &#39;мунісны&#39;, &#39;муніны&#39;, &#39;муніныс&#39;)) filter_verbs &lt;- function(data){ data %&gt;% mutate(type = if_else(str_detect(token, &#39;сн..$&#39;) &amp; str_detect(token, &#39;с$&#39;), true = &#39;isnɨs&#39;, false = if_else(str_detect(token, &#39;сн.&#39;) &amp; str_detect(token, &#39;ы$&#39;), true = &#39;isnɨ&#39;, false = if_else(str_detect(token, &#39;([^с]ныс$)&#39;), true = &#39;inɨs&#39;, false = &#39;inɨ&#39;)))) %&gt;% mutate(type_final = as.factor(if_else(str_detect(type, &#39;s$&#39;), true = &#39;s-final&#39;, false = &#39;vowel-final&#39;))) %&gt;% mutate(type_medial = as.factor(if_else(str_detect(type, &#39;sn&#39;), true = &#39;s-medial&#39;, false = &#39;vowel-medial&#39;))) } dummy_data %&gt;% filter_verbs() %&gt;% knitr::kable() token type type_final type_medial мунісныс isnɨs s-final s-medial мунісны isnɨ vowel-final s-medial муніны inɨ vowel-final vowel-medial муніныс inɨs s-final vowel-medial It seems that the result is correct, so we can start to apply it to the real data. However, if we find problems along the way, we can always return to this point and modify the function, as long as we haven’t started to do manual edits in the derived files. In the next step we load the corpus directly into R from previously saved RDS file. How these can be worked with was described in section Tools: R. kpv &lt;- read_rds(&#39;corpus.rds&#39;) Of course if the corpus is small enough we can also just read it directly from whatever source we have. However, as this process is repeated every time we compile this website, it is easier to read it this way. verbs &lt;- kpv %&gt;% filter(! participant %in% c(&#39;NTP-M-1986&#39;, &#39;MR-M-1974&#39;, &#39;RB-1974&#39;)) %&gt;% # this simply removes the western researchers Niko Partanen, Michael Rießler and Rogier Blokland filter(str_detect(token, &#39;(и|і)(с)?ны(с)?$&#39;)) %&gt;% # This selects the wanted tokens filter_verbs() %&gt;% # Here we call the function we set up above select(token, type, everything()) We can easily look into how many types we have and which are the most common tokens. This doesn’t modify the data frame, but gives us information about how sensical the result is. Usually this kind of more finished document doesn’t show the whole workflow how the preprocessing function was edited iteratively while examination of the results shows something is off, but in principle older versions should usually be present in older Git commits. This can be useful when it is realized later that some of the older version was indeed the correct one. verbs %&gt;% count(type) %&gt;% slice(1:10) %&gt;% knitr::kable() type n inɨ 300 inɨs 250 isnɨ 1803 isnɨs 1196 verbs %&gt;% count(token) %&gt;% arrange(desc(n)) %&gt;% slice(1:10) %&gt;% knitr::kable() token n вӧліны 176 вӧліныс 113 кучисныс 84 воисныс 67 олісны 67 шуисны 57 локтісны 46 ветлісны 45 карисны 44 олісныс 44 verbs %&gt;% filter(type == &#39;inɨs&#39;) %&gt;% slice(1:10) %&gt;% knitr::kable() token type utterance reference participant time_start time_end session_name filename word type_final type_medial муніныс inɨs Нужник бӧкас сулалэ и бӧр муніныс. kpv_izva19300000ArtijevI-135-10 IXA-M-18XX 90000 100000 kpv_izva19300000ArtijevI-135 /Volumes/langdoc/langs/kpv/kpv_izva19300000ArtijevI-135/kpv_izva19300000ArtijevI-135.eaf муніныс s-final vowel-medial муніныс inɨs Муніныс ныа вӧлӧн. kpv_izva19570000-290_3bz-11 XXV-M-19XX 52900 54220 kpv_izva19570000-290_3bz-Bakur /Volumes/langdoc/langs/kpv/kpv_izva19570000-290_3bz-Bakur/kpv_izva19570000-290_3bz-Bakur.eaf Муніныс s-final vowel-medial вӧліныс inɨs Баракъясас вӧліныс гразнӧйӧсь зэй. kpv_izva19570000-290_3bz-23 XXV-M-19XX 111380 114420 kpv_izva19570000-290_3bz-Bakur /Volumes/langdoc/langs/kpv/kpv_izva19570000-290_3bz-Bakur/kpv_izva19570000-290_3bz-Bakur.eaf вӧліныс s-final vowel-medial лэччаніныс inɨs Рытнас шонді лэччаніныс ке сӧстэм - мӧдасылас лоэ шондіа лун , kpv_izva19590000IgusevJA-280 JAI-M-1939 1104086 1109573 kpv_izva19590000IgusevJA /Volumes/langdoc/langs/kpv/kpv_izva19590000IgusevJA/kpv_izva19590000IgusevJA.eaf лэччаніныс s-final vowel-medial лэччаніныс inɨs Шонді лэччаніныс ке гӧрд , мӧдасылас лоас тӧла лун . kpv_izva19590000IgusevJA-288 JAI-M-1939 1165648 1169283 kpv_izva19590000IgusevJA /Volumes/langdoc/langs/kpv/kpv_izva19590000IgusevJA/kpv_izva19590000IgusevJA.eaf лэччаніныс s-final vowel-medial уудиныс inɨs уудиныс (водьпомыс) ляпкыд. kpv_izva19591216-05582_2az.29 MXV-F-1937 147886 150661 kpv_izva19591216-05582_2az /Volumes/langdoc/langs/kpv/kpv_izva19591216-05582_2az/kpv_izva19591216-05582_2az.eaf уудиныс s-final vowel-medial лоиныс inɨs а сыри- сыритяяс ӧні нин лоиныс важынкаяс: kpv_izva19591216-05582_4a.078 MXV-F-1937 519866 526876 kpv_izva19591216-05582_4a /Volumes/langdoc/langs/kpv/kpv_izva19591216-05582_4a/kpv_izva19591216-05582_4a.eaf лоиныс s-final vowel-medial кӧйиныс inɨs кӧйиныс сёяс и кулэ. kpv_izva19591216-05582_4a.101 MXV-F-1937 717843 720943 kpv_izva19591216-05582_4a /Volumes/langdoc/langs/kpv/kpv_izva19591216-05582_4a/kpv_izva19591216-05582_4a.eaf кӧйиныс s-final vowel-medial кӧйиныс inɨs кӧйиныс верме кӧрлы джагедны и луна. kpv_izva19591216-05582_4a.112 MXV-F-1937 774030 779556 kpv_izva19591216-05582_4a /Volumes/langdoc/langs/kpv/kpv_izva19591216-05582_4a/kpv_izva19591216-05582_4a.eaf кӧйиныс s-final vowel-medial кӧйиныс inɨs кӧйиныс дебсе росся костэ; kpv_izva19591216-05582_4a.113 MXV-F-1937 779556 783611 kpv_izva19591216-05582_4a /Volumes/langdoc/langs/kpv/kpv_izva19591216-05582_4a/kpv_izva19591216-05582_4a.eaf кӧйиныс s-final vowel-medial Often the actual benefit of having the data in a programmatic environment instead of just ELAN is that can use in the research variables from metadata that cannot be accessed in ELAN. In our participant naming system we use usually the convention where the gender and birthyear are marked to the name id. verbs &lt;- verbs %&gt;% mutate(gender = str_extract(participant, &#39;(?&lt;=-)[MF](?=-)&#39;)) %&gt;% select(token, type, gender, participant, everything()) verbs %&gt;% count(gender) ## # A tibble: 3 x 2 ## gender n ## &lt;chr&gt; &lt;int&gt; ## 1 F 2455 ## 2 M 1029 ## 3 &lt;NA&gt; 65 Here we see that for some speakers we didn’t find the gender specified as is described in the convention. The reason is that there are some old transcribed texts for which we don’t know who is the speaker – very likely the same text has been elicitated from several speakers and is some kind of a synthesis of those. We simply don’t know. This is somewhat typical for fieldwork data from the 19th and early 20th century, although variation in practices is also great. In this case we know (after looking a bit better which files are having this problem) what is the reason for this problem, and we can decide to leave out those cases if necessary. However, it is good to take into account that if there are missing values, it is very important to examine what is going on behind them. verbs %&gt;% filter(is.na(gender)) %&gt;% count(type) ## # A tibble: 4 x 2 ## type n ## &lt;chr&gt; &lt;int&gt; ## 1 inɨ 7 ## 2 inɨs 3 ## 3 isnɨ 52 ## 4 isnɨs 3 In this point we can take a note that in the oldest data available there are very few examples of the s-final forms, but it is also a very small subset of the corpus. Before advancing further, we can add one more variable to the dataframe we are working with. verbs &lt;- verbs %&gt;% mutate(year = str_extract(session_name, &#39;\\\\d{4}(?=\\\\d{4})&#39;)) %&gt;% select(token, type, gender, year, everything()) Year is of course a bit problematic variable as we aren’t really having that much data for each year. So let’s add a new column for the decade. verbs &lt;- verbs %&gt;% mutate(type = as.factor(type)) %&gt;% mutate(year = as.numeric(year)) %&gt;% mutate(decade = (year %/% 10) * 10) %&gt;% select(token, type, gender, year, decade, everything()) After this the plotting will work nicely. We can analyze the distribution of tokens per decade: ggplot(verbs, aes(x = decade)) + geom_bar() This reflects well the data distribution of IKDP corpus, which makes sense as these verb forms should occur everywhere. ggplot(verbs) + geom_bar(mapping = aes(x = decade, fill = type_medial), position = &quot;fill&quot;) ggplot(verbs %&gt;% filter(year &gt; 1930)) + # This leaves older data out as it is so gappy geom_bar(mapping = aes(x = decade, fill = type_medial), position = &quot;fill&quot;) ggplot(verbs %&gt;% filter(year &gt; 1930) %&gt;% filter(! is.na(gender))) + # If we want to use a variable later, we have to make sure it is available geom_bar(mapping = aes(x = decade, fill = type_medial), position = &quot;fill&quot;) + facet_grid(. ~ gender) Next thing I want to try is to associate birth places with areas and plot those. For this we need bit more metadata, which I’m now reading from our Filemaker Pro database, but which should be set up better for the actual teaching. Once we have set up the processing workflow to the point where we have something useful, and maybe get into phase where we don’t know what we are doing, it can be an useful practice to write the dataframe into a new variable so that it is not necessary to do all previous changes when something goes wrong. In this case I create a new variable called verbs_test, and when we accidentally do something we didn’t want to it is easy to run the code from this point onward. source(&#39;/Volumes/langdoc/langs/kpv/FM_meta.R&#39;) ## Loading required package: DBI ## Loading required package: rJava ## Joining, by = &quot;Actor_ID&quot; ## Joining, by = &quot;Session_ID&quot; ## Joining, by = &quot;RecPlace_OSM_ID&quot; ## Joining, by = &quot;PlaceofRes_OSM_ID&quot; ## Joining, by = &quot;Birthplace_OSM_ID&quot; ## Warning in eval(ei, envir): NAs introduced by coercion ## Warning in eval(ei, envir): NAs introduced by coercion ## Warning in eval(ei, envir): NAs introduced by coercion ## Warning in eval(ei, envir): NAs introduced by coercion ## Warning in eval(ei, envir): NAs introduced by coercion ## Warning in eval(ei, envir): NAs introduced by coercion ## Warning in eval(ei, envir): NAs introduced by coercion # meta %&gt;% distinct(place_birth, birthplace_osm_id, lat_birth, lon_birth) %&gt;% # arrange(place_birth) %&gt;% # group_by(place_birth) %&gt;% # filter(n()&gt;1) verbs_test &lt;- left_join(verbs, meta %&gt;% distinct(participant, lat_birth, lon_birth, attr_foreign) %&gt;% filter(participant %in% verbs$participant)) %&gt;% rename(lat = lat_birth, lon = lon_birth) ## Joining, by = &quot;participant&quot; verbs_test &lt;- verbs_test %&gt;% mutate(variety = str_extract(session_name, &#39;(?&lt;=kpv_)[a-z]+&#39;)) 12.1 From points to polygon #izva &lt;- st_read(&#39;https://raw.githubusercontent.com/langdoc/IKDP-2/025e817c25181b683661a21ab36facb63c830604/data/izva_dialects.geojson&#39;) izva &lt;- st_read(&#39;/Users/niko/github/IKDP-2/data/izva_dialects-test.geojson&#39;) ## Reading layer `OGRGeoJSON&#39; from data source `/Users/niko/github/IKDP-2/data/izva_dialects-test.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 19 features and 4 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 28.96325 ymin: 55.94814 xmax: 74.33177 ymax: 70.39286 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ggplot(izva) + geom_sf(aes(fill = variant)) + geom_point(data = verbs_test %&gt;% filter(! is.na(lon)), aes(x = lon, y = lat)) verbs_test %&gt;% filter(is.na(lat)) %&gt;% count(participant) %&gt;% arrange(desc(n)) ## # A tibble: 31 x 2 ## participant n ## &lt;chr&gt; &lt;int&gt; ## 1 IIB-M-1946 58 ## 2 TFA-F-1934 46 ## 3 APP-F-1957 39 ## 4 unknown 25 ## 5 AAZ-F-1940 20 ## 6 KOM-F-1964 13 ## 7 XXC-F-196X 8 ## 8 NGK-F-1956 6 ## 9 группа 6 ## 10 S1 5 ## # ... with 21 more rows ## based on this: ## https://gis.stackexchange.com/questions/222978/lon-lat-to-simple-features-sfg-and-sfc-in-r geo_inside &lt;- function(lon, lat, map, variable) { variable &lt;- enquo(variable) pt &lt;- tibble::data_frame(x = lon, y = lat) %&gt;% st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;), crs = st_crs(map)) pt %&gt;% st_join(map) %&gt;% pull(!!variable) } verbs_test &lt;- verbs_test %&gt;% filter(! is.na(lon) | ! is.na(lat)) verbs_test %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 3281 verbs_test &lt;- verbs_test %&gt;% mutate(region = geo_inside(lon, lat, izva, variant)) ## although coordinates are longitude/latitude, it is assumed that they are planar verbs_test &lt;- verbs_test %&gt;% mutate(dialect = geo_inside(lon, lat, izva, dialect)) ## although coordinates are longitude/latitude, it is assumed that they are planar ggplot(data = verbs_test %&gt;% filter(str_detect(dialect, &#39;zva&#39;)), aes(x = type)) + geom_bar() + facet_wrap(region ~ gender) verbs_test %&gt;% count(dialect) ## # A tibble: 9 x 2 ## dialect n ## &lt;fctr&gt; &lt;int&gt; ## 1 Central Sysola 6 ## 2 izva 1311 ## 3 Izva 1566 ## 4 Lower Vychegda 1 ## 5 Luza-Letka 12 ## 6 Syktyvdin 7 ## 7 Udora 257 ## 8 Upper Sysola 30 ## 9 Upper Vychegda 91 ggplot(verbs_test %&gt;% filter(! is.na(region)) %&gt;% filter(str_detect(dialect, &#39;zva&#39;))) + geom_bar(mapping = aes(x = region, fill = type), position = &quot;fill&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) ggplot(verbs_test %&gt;% filter(! participant %in% c(&#39;MSF-F-1968&#39;, &#39;VPC-M-1993&#39;)) %&gt;% filter(! is.na(region)) %&gt;% filter(str_detect(dialect, &#39;zva&#39;))) + geom_bar(mapping = aes(x = region, fill = type), position = &quot;fill&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) library(geofacet) mygrid &lt;- data.frame( code = c(&quot;IZKA&quot;, &quot;IZKP&quot;, &quot;IZBT&quot;, &quot;IZCO&quot;, &quot;IZSIB&quot;, &quot;IZEX&quot;, &quot;IZKU&quot;, &quot;UDVA&quot;, &quot;UDMZ&quot;, &quot;VM&quot;, &quot;IZUP&quot;, &quot;PE&quot;, &quot;VYLO&quot;, &quot;VYUP&quot;, &quot;SK&quot;, &quot;SD&quot;, &quot;SC&quot;, &quot;LL&quot;, &quot;SU&quot;), name = c(&quot;Kanin&quot;, &quot;Kola Peninsula&quot;, &quot;Tundra&quot;, &quot;Izhma core&quot;, &quot;Siberia&quot;, &quot;Izhma extension&quot;, &quot;Kolva-Usa&quot;, &quot;Vashka&quot;, &quot;Mezen&quot;, &quot;Vym&quot;, &quot;Upper Izhma&quot;, &quot;Pechora&quot;, &quot;Lower Vychegda&quot;, &quot;Upper Vychegda&quot;, &quot;Syktyvkar&quot;, &quot;Syktyvdin&quot;, &quot;Central Sysola&quot;, &quot;Luza-Letka&quot;, &quot;Upper Sysola&quot;), row = c(1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 5, 6, 7, 7, 8), col = c(2, 1, 3, 4, 7, 5, 6, 1, 2, 3, 4, 5, 3, 4, 3, 3, 4, 2, 5), stringsAsFactors = FALSE ) geofacet::grid_preview(mygrid) ## You provided a user-specified grid. If this is a generally-useful ## grid, please consider submitting it to become a part of the ## geofacet package. You can do this easily by calling: ## grid_submit(__grid_df_name__) # verbs_test %&gt;% mutate(name = as.character(region)) %&gt;% # mutate(name = if_else(name == &#39;Bolshezyemelskaya Tundra&#39;, &#39;Tundra&#39;, name)) %&gt;% # left_join(mygrid) %&gt;% # filter(! is.na(region)) %&gt;% filter(variety == &#39;vym&#39;) %&gt;% select(name) ggplot(verbs_test %&gt;% mutate(name = as.character(region)) %&gt;% mutate(name = if_else(name == &#39;Bolshezyemelskaya Tundra&#39;, &#39;Tundra&#39;, name)) %&gt;% left_join(mygrid) %&gt;% filter(! is.na(region)) %&gt;% filter(! participant %in% c(&#39;MSF-F-1968&#39;, &#39;VPC-M-1993&#39;, &#39;VVF-F-1957&#39;))) + geom_bar(mapping = aes(x = factor(&quot;&quot;), fill = type), position = &quot;fill&quot;) + facet_geo(~ name, grid = mygrid) + labs(title = &quot;First preterite plural verb allomorphs in Komi-Zyrian dialects&quot;, subtitle = &quot;Map approximates the location and contact relations of the dialects. For blanks no data available.&quot;, caption = &quot;Work done in LATTICE, Paris\\nData Source: IKDP Author: Niko Partanen (2017)&quot;, y = &quot;Percentage of different types&quot;, x = &quot;&quot;) + theme_bw() + theme(axis.line=element_blank(), axis.text.x=element_blank(), axis.text.y=element_blank(), axis.ticks=element_blank(), # axis.title.x=element_blank(), # axis.title.y=element_blank(), # legend.position=&quot;none&quot;, panel.background=element_blank(), # panel.border=element_blank(), panel.grid.major=element_blank(), panel.grid.minor=element_blank()) + theme(strip.background = element_rect(fill=&quot;white&quot;, linetype = &#39;blank&#39;))+ theme(strip.text = element_text(colour = &#39;black&#39;, size = 7)) ## Joining, by = &quot;name&quot; ## You provided a user-specified grid. If this is a generally-useful ## grid, please consider submitting it to become a part of the ## geofacet package. You can do this easily by calling: ## grid_submit(__grid_df_name__) # ggplot(verbs_test %&gt;% # filter(! participant %in% c(&#39;MSF-F-1968&#39;, &#39;VPC-M-1993&#39;)) %&gt;% # filter(! is.na(region)) %&gt;% # filter(! is.na(gender)) %&gt;% # filter(str_detect(dialect, &#39;zva&#39;))) + # geom_bar(mapping = aes(x = gender, fill = type), position = &quot;fill&quot;) + # theme(axis.text.x = element_text(angle = 90, hjust = 1)) # ggplot(verbs_test %&gt;% filter(! is.na(region)) %&gt;% filter(! str_detect(dialect, &#39;zva&#39;))) + geom_bar(mapping = aes(x = region, fill = type), position = &quot;fill&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) # verbs_test %&gt;% filter(type == &#39;inɨ&#39;) verb_stems &lt;- verbs_test %&gt;% mutate(stem = str_extract(token, &#39;.+(?=(и|і)с?ны?с?)&#39;)) %&gt;% distinct(stem) %&gt;% arrange(stem) # verb_stems %&gt;% # filter(! str_detect(stem, &quot;(j|[&#39;])&quot;)) %&gt;% # write_csv(&#39;data/izva_verbs.csv&#39;) # verbs_test %&gt;% # mutate(stem = str_extract(token, &#39;.+(?=(и|і)с?ны?с?)&#39;)) %&gt;% # count(stem) %&gt;% # arrange(desc(n)) %&gt;% # slice(10:20) # verbs_test %&gt;% # filter(variety == &#39;izva&#39;) %&gt;% # mutate(stem = str_extract(token, &#39;.+(?=(и|і)с?ны?с?)&#39;)) %&gt;% # filter(! stem == &#39;в&#39;) %&gt;% # count(stem, type) %&gt;% # rename(hits = n) %&gt;% # arrange(stem) %&gt;% # split(.$stem) %&gt;% # map(~ mutate(.x, diff_types = n()) %&gt;% # mutate(type_ratio = hits / sum(hits)) %&gt;% # mutate(sum_hits = sum(hits))) %&gt;% # bind_rows %&gt;% # filter(sum_hits &gt; 10) %&gt;% # arrange(desc(diff_types)) %&gt;% View # verbs_test %&gt;% # filter(variety == &#39;izva&#39;) %&gt;% # mutate(stem = str_extract(token, &#39;.+(?=(и|і)с?ны?с?)&#39;)) %&gt;% # filter(stem == &#39;босьт&#39;) %&gt;% # filter(type == &#39;isnɨs&#39;) %&gt;% # select(utterance) %&gt;% # View # verbs_test %&gt;% filter(str_detect(utterance, &#39;всю&#39;)) %&gt;% select(utterance, filename) %&gt;% open_eaf(3) # kpv %&gt;% filter(str_detect(utterance, &#39;кеде &#39;)) %&gt;% distinct(utterance, participant, filename) # # split(.$type) %&gt;% # map(~ count(.x, stem) %&gt;% # arrange(desc(n)) %&gt;% # slice(10:20)) # verbs_test %&gt;% filter(str_detect(token, &#39;^кор&#39;)) %&gt;% select(utterance, participant, variety, year) # verbs_test %&gt;% filter(str_detect(token, &#39;^торйед&#39;)) %&gt;% open_eaf(1) verbs_test %&gt;% mutate(stem = str_extract(token, &#39;.+(?=(и|і)с?ны?с?)&#39;)) %&gt;% left_join(read_csv(&#39;data/izva_verbs.csv&#39;)) %&gt;% filter(! is.na(category)) %&gt;% filter(stem != &#39;вӧл&#39;) %&gt;% # select(token, type, type_medial, gender, year, participant, category, variety, region, dialect) %&gt;% ggplot(data = ., aes(x = category)) + geom_bar() + facet_grid(. ~ type) ## Parsed with column specification: ## cols( ## stem = col_character(), ## category = col_character(), ## remove = col_character() ## ) ## Joining, by = &quot;stem&quot; verbs_test %&gt;% mutate(stem = str_extract(token, &#39;.+(?=(и|і)с?ны?с?)&#39;)) %&gt;% left_join(read_csv(&#39;data/izva_verbs.csv&#39;)) %&gt;% filter(! is.na(category)) %&gt;% filter(dialect %in% c(&#39;Udora&#39;, &#39;izva&#39;, &#39;Izva&#39;)) %&gt;% filter(stem != &#39;вӧл&#39;) %&gt;% select(token, type, gender, year, participant, category, variety, region, dialect) %&gt;% ggplot(data = ., aes(x = category)) + geom_bar() + facet_grid(type ~ variety) ## Parsed with column specification: ## cols( ## stem = col_character(), ## category = col_character(), ## remove = col_character() ## ) ## Joining, by = &quot;stem&quot; int &lt;- verbs_test %&gt;% mutate(stem = str_extract(token, &#39;.+(?=(и|і)с?ны?с?)&#39;)) %&gt;% left_join(read_csv(&#39;data/izva_verbs.csv&#39;)) %&gt;% filter(! is.na(category)) ## Parsed with column specification: ## cols( ## stem = col_character(), ## category = col_character(), ## remove = col_character() ## ) ## Joining, by = &quot;stem&quot; # int %&gt;% # left_join(count(int, stem) %&gt;% rename(token_count = n)) %&gt;% # arrange(desc(token_count)) %&gt;% # distinct(stem, token_count) %&gt;% # ggplot(data = ., # aes(x = token, y = token_count)) + # geom_bar() # блиныс # verbs %&gt;% arrange(token) %&gt;% distinct(token) %&gt;% write_csv(&#39;data/izva_verbs.csv&#39;) # verbs %&gt;% filter(str_detect(token, &#39;j&#39;)) # verbs %&gt;% mutate(variant = str_extract(session_name, &#39;(?&lt;=kpv_)[a-z]+(?=\\\\d)&#39;)) %&gt;% count(variant) #verbs %&gt;% left_join() "],
["final-words.html", "Section 13 Final Words", " Section 13 Final Words We are living in very interesting times where many technologies are just making their appearance. It is certain that our work practices will also shift and change. If we are working with smaller or less standardized languages, the things will certainly take their time before everything is available for us as well. It is thereby hard to draw a line between the problems which we can label solved and unsolved, because even if something is considered solved, it may not work at all with our languages. However, I think following parts we commonly work with will in some point be automatically done: Segmentation Speaker recognition Speech recognition POS tagging Syntactic annotation Named Entity Recognition … What is important to see here, is that this course did not touch any of these topics as such, but we have been only discussing the possible ways to analyse and manipulate the data which we have got one way or another, usually through longer manual work. Having more data automatically will not change the need to be able to manipulate and analyse the data we have, on contrary, it certainly will require even more knowledge about these issues than now, as the datasets become too large to be manually adjusted or examined. "],
["references.html", "References", " References "]
]
